{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "from util import *\n",
    "import torch\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 439312 word sense data from White et. al., 2016.\n",
      "Parsed 12543 training data from UD_English-EWT/en_ewt-ud-train.conllu.\n",
      "Parsed 2077 testing data from UD_English-EWT/en_ewt-ud-test.conllu\n",
      "Parsed 2002 dev data from UD_English-EWT/en_ewt-ud-dev.conllu\n",
      "Parsed 20 sentences\n",
      "******************* Data Example ***********************\n",
      "Sentence: ['on', 'August', '9', ',', '2004', ',', 'it', 'be', 'announce', 'that', 'in', 'the', 'spring', 'of', '2001', ',', 'a', 'man', 'name', 'El', '-', 'Shukrijumah', ',', 'also', 'know', 'as', 'Jafar', 'the', 'Pilot', ',', 'who', 'be', 'part', 'of', 'a', '\"', 'second', 'wave', ',', '\"', 'have', 'be', 'case', 'New', 'York', 'City', 'helicopter', '.']\n",
      "Target Word Index: 12\n",
      "Target Word Sense (index in WordNet 3.1): spring.n.01\n",
      "********************************************************\n",
      "******************* fine-tuning MLP structure ***********************\n",
      "Current Task: WSD\n",
      "Linear(in_features=512, out_features=300, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=300, out_features=10, bias=True)\n",
      "Softmax()\n",
      "torch.Size([300, 512])\n",
      "torch.Size([300])\n",
      "torch.Size([10, 300])\n",
      "torch.Size([10])\n",
      "torch.Size([256, 3072])\n",
      "torch.Size([256])\n",
      "torch.Size([1024, 256])\n",
      "torch.Size([1024, 256])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 256])\n",
      "torch.Size([1024, 256])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([1024, 256])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024, 512])\n",
      "torch.Size([1024, 256])\n",
      "torch.Size([1024])\n",
      "torch.Size([1024])\n",
      "Model(\n",
      "  (layers): ModuleDict(\n",
      "    (WSD): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=300, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=300, out_features=10, bias=True)\n",
      "      (3): Softmax()\n",
      "    )\n",
      "  )\n",
      "  (mlp_dropout): Dropout(p=0)\n",
      "  (tuned_embed_MLP): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (rnn): LSTM(256, 256, num_layers=2, bidirectional=True)\n",
      ")\n",
      "**********************************************************************\n",
      "\n",
      "Original ELMo embeddings size: torch.Size([20, 3, 48, 1024]), mask: torch.Size([20, 48])\n",
      "Embedding size after dimension reduction: torch.Size([48, 20, 256]), mask: torch.Size([48, 20, 256])\n",
      "\n",
      "Embedding size after bi-LSTM: torch.Size([48, 20, 512])\n",
      "\n",
      "Output of the fine-tuning MLP: \n",
      "Total 20 words. \n",
      "Each word has a 10-d vector output as probabilities distributing over its senses: torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0985, 0.1047, 0.1024, 0.0982, 0.0936, 0.1035, 0.0998, 0.0961,\n",
       "         0.1006], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1030, 0.0984, 0.1040, 0.1035, 0.0974, 0.0939, 0.1047, 0.0992, 0.0945,\n",
       "         0.1014], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1030, 0.0984, 0.1040, 0.1035, 0.0974, 0.0939, 0.1047, 0.0992, 0.0945,\n",
       "         0.1014], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0989, 0.1053, 0.1031, 0.0973, 0.0946, 0.1040, 0.0992, 0.0946,\n",
       "         0.1005], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0989, 0.1053, 0.1031, 0.0973, 0.0946, 0.1040, 0.0992, 0.0946,\n",
       "         0.1005], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0989, 0.1053, 0.1031, 0.0973, 0.0946, 0.1040, 0.0992, 0.0946,\n",
       "         0.1005], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1026, 0.0989, 0.1053, 0.1031, 0.0973, 0.0946, 0.1040, 0.0992, 0.0946,\n",
       "         0.1005], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1024, 0.0991, 0.1053, 0.1028, 0.0979, 0.0938, 0.1042, 0.0992, 0.0947,\n",
       "         0.1005], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.1024, 0.0991, 0.1053, 0.1028, 0.0979, 0.0938, 0.1042, 0.0992, 0.0947,\n",
       "         0.1005], grad_fn=<SoftmaxBackward>)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# parse the data\n",
    "wsd_data, train_data, test_data, dev_data = parse_data()\n",
    "\n",
    "# return the raw sentences from the EUD for train, test, and dev\n",
    "# test the first 20 sentences\n",
    "train_sentences, train_word_sense, train_word_index, test_sentences, test_word_sense, test_word_index, dev_sentences, dev_word_sense, dev_word_index = get_raw_sentences(wsd_data, train_data, test_data, dev_data, 20)\n",
    "\n",
    "# ELMo setup\n",
    "# ELMo is tuned to lower dimension (256) by MLP in Model\n",
    "elmo = ElmoEmbedder()\n",
    "model = Model(elmo_class = elmo)\n",
    "\n",
    "# MLP illustration\n",
    "print_fine_tuning_MLP(model, 'WSD')\n",
    "\n",
    "# forward propagation\n",
    "# ELMo (1024) -> dimension reduction (256) -> bi-LSTM (512) -> fine-tuning MLP (10)\n",
    "model.forward(train_sentences, train_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
