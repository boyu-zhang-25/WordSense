{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import string\n",
    "import itertools\n",
    "from io import open\n",
    "from conllu import parse_incr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3661, -0.1964, -0.7226, -0.2218,  0.4981]])\n",
      "tensor([[ 0.3661],\n",
      "        [-0.1964],\n",
      "        [-0.7226],\n",
      "        [-0.2218],\n",
      "        [ 0.4981]])\n",
      "tensor(0.9193)\n",
      "0.9192752838134766\n",
      "tensor([0.9193])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import CosineEmbeddingLoss\n",
    "l = torch.nn.CosineEmbeddingLoss()\n",
    "cs = torch.nn.CosineSimilarity(dim = 1)\n",
    "v1 = torch.randn(1, 5)\n",
    "v2 = torch.randn(1, 5)\n",
    "\n",
    "print(v1)\n",
    "print(v1.view(5, 1))\n",
    "\n",
    "def ang(a, b):\n",
    "    return torch.dot(a, b) / (torch.norm(a) * torch.norm(b))\n",
    "\n",
    "print(1 - ang(v1[0], v2[0]))\n",
    "y = torch.ones(1)\n",
    "print(l(v1, v2, y).item())\n",
    "print(1 - cs(v1, v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the WSD dataset first\n",
    "\n",
    "'''\n",
    "Copyright@\n",
    "White, A. S., D. Reisinger, K. Sakaguchi, T. Vieira, S. Zhang, R. Rudinger, K. Rawlins, & B. Van Durme. 2016. \n",
    "[Universal decompositional semantics on universal dependencies]\n",
    "(http://aswhite.net/media/papers/white_universal_2016.pdf). \n",
    "To appear in *Proceedings of the Conference on Empirical Methods in Natural Language Processing 2016*.\n",
    "'''\n",
    "\n",
    "def parse_wsd_data():\n",
    "\n",
    "    # parse the WSD dataset\n",
    "    wsd_data = []\n",
    "\n",
    "    # read in tsv by White et. al., 2016\n",
    "    with open('data/wsd/wsd_eng_ud1.2_10262016.tsv', mode = 'r') as wsd_file:\n",
    "\n",
    "        tsv_reader = csv.DictReader(wsd_file, delimiter = '\\t')      \n",
    "\n",
    "        # store the data: ordered dict row\n",
    "        for row in tsv_reader:                                \n",
    "\n",
    "            # each data vector\n",
    "            wsd_data.append(row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print('Parsed {} word sense data from White et. al., 2016.'.format(len(wsd_data)))\n",
    "\n",
    "    return wsd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 439312 word sense data from White et. al., 2016.\n"
     ]
    }
   ],
   "source": [
    "# get the raw wsd data\n",
    "wsd_data = parse_wsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return: \n",
    "all senses for each word \n",
    "all definitions for each word\n",
    "all supersenses\n",
    "from the EUD for train, test, and dev dataset\n",
    "index provided by WSD dataset by White et. al.\n",
    "'''\n",
    "# get all the senses and definitions for each word from WSD dataset\n",
    "# order of senses and definitions are in order\n",
    "def get_all_senses_and_definitions(wsd_data):\n",
    "\n",
    "    # all senses for each word in train and dev\n",
    "    # supersense is shared \n",
    "    all_senses = {}\n",
    "    all_definitions = {}\n",
    "    all_supersenses = {}\n",
    "\n",
    "    # all senses for each word in test\n",
    "    all_test_senses = {}\n",
    "    all_test_definitions = {}\n",
    "    \n",
    "    # only get the senses for train and dev set\n",
    "    for i in range(len(wsd_data)):\n",
    "        \n",
    "        # get the original sentence from EUD\n",
    "        sentence_id = wsd_data[i].get('Sentence.ID')\n",
    "        \n",
    "        # get the definitions for the target word from EUD\n",
    "        definition = wsd_data[i].get('Sense.Definition').split(' ')\n",
    "        \n",
    "        # the index in EUD is 1-based!!!\n",
    "        sentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "        word_index = int(wsd_data[i].get('Arg.Token')) - 1\n",
    "        word_lemma = wsd_data[i].get('Arg.Lemma')\n",
    "        word_sense = wsd_data[i].get('Synset')\n",
    "        response = wsd_data[i].get('Sense.Response')\n",
    "        \n",
    "        # senses for train and dev\n",
    "        # preserve unknown words\n",
    "        if wsd_data[i].get('Split') != 'test':\n",
    "        \n",
    "            # supersense-> (word_lemma, word_sense) dictionary\n",
    "            super_s = wn.synset(word_sense).lexname().replace('.', '_')\n",
    "            if all_supersenses.get(super_s, 'not_exist') != 'not_exist':\n",
    "                all_supersenses[super_s].add((word_lemma, word_sense))\n",
    "            else:\n",
    "                all_supersenses[super_s] = {(word_lemma, word_sense)}\n",
    "\n",
    "            # if the word already exits: add the new sense to the list\n",
    "            # else: creata a new list for the word\n",
    "            if all_senses.get(word_lemma, 'not_exist') != 'not_exist':\n",
    "                if word_sense not in all_senses[word_lemma]:\n",
    "                    all_senses[word_lemma].append(word_sense)\n",
    "            else:\n",
    "                all_senses[word_lemma] = []\n",
    "                all_senses[word_lemma].append(word_sense)            \n",
    "            \n",
    "            if all_definitions.get(word_lemma,'not_exist') != 'not_exist':\n",
    "                if definition not in all_definitions[word_lemma]: \n",
    "                    all_definitions[word_lemma].append(definition)\n",
    "            else:\n",
    "                all_definitions[word_lemma] = []\n",
    "                all_definitions[word_lemma].append(definition)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            # all the senses and definitions for test words\n",
    "            if all_test_senses.get(word_lemma, 'not_exist') != 'not_exist':\n",
    "                if word_sense not in all_test_senses[word_lemma]:\n",
    "                    all_test_senses[word_lemma].append(word_sense)\n",
    "            else:\n",
    "                all_test_senses[word_lemma] = []\n",
    "                all_test_senses[word_lemma].append(word_sense)            \n",
    "            \n",
    "            if all_test_definitions.get(word_lemma,'not_exist') != 'not_exist':\n",
    "                if definition not in all_test_definitions[word_lemma]: \n",
    "                    all_test_definitions[word_lemma].append(definition)\n",
    "            else:\n",
    "                all_test_definitions[word_lemma] = []\n",
    "                all_test_definitions[word_lemma].append(definition)            \n",
    "        \n",
    "    return all_senses, all_definitions, all_supersenses, all_test_senses, all_test_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the senses and definitions\n",
    "all_senses, all_definitions, all_supersenses, all_test_senses, all_test_definitions = get_all_senses_and_definitions(wsd_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "double\n",
      "training\n",
      "half\n",
      "pop\n",
      "update\n",
      "train\n"
     ]
    }
   ],
   "source": [
    "# get rid of build-in attr of torch\n",
    "pdict = nn.ParameterDict()\n",
    "# print(set(pdict.__dir__()))\n",
    "# print(set(all_senses.keys()))\n",
    "attr_set = set(all_senses.keys()).intersection(set(pdict.__dir__()))\n",
    "for attr in attr_set:\n",
    "    print(attr)\n",
    "    del all_senses[attr]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe specific Synset method is lexname, e.g. wn.synsets('spring')[0].lexname(). \\nThat should make it really easy to get the suspersenses.\\nAnd if you have the synset name–e.g. 'spring.n.01'\\nyou can access the supersense directly: wn.synset('spring.n.01').lexname().\\nWhich returns 'noun.time'.\\nAnd wn.synset('spring.n.02').lexname() returns 'noun.artifact'\\n\\nfor idx, d in enumerate(all_definitions['spring']):\\n    print(d)\\n    print(wn.synset(all_senses['spring'][idx]).lexname())\\n\\nfor _ in wn.synsets('spring'):\\n    print(_.lexname())\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for the WordNet NLTK API\n",
    "'''\n",
    "The specific Synset method is lexname, e.g. wn.synsets('spring')[0].lexname(). \n",
    "That should make it really easy to get the suspersenses.\n",
    "And if you have the synset name–e.g. 'spring.n.01'\n",
    "you can access the supersense directly: wn.synset('spring.n.01').lexname().\n",
    "Which returns 'noun.time'.\n",
    "And wn.synset('spring.n.02').lexname() returns 'noun.artifact'\n",
    "\n",
    "for idx, d in enumerate(all_definitions['spring']):\n",
    "    print(d)\n",
    "    print(wn.synset(all_senses['spring'][idx]).lexname())\n",
    "\n",
    "for _ in wn.synsets('spring'):\n",
    "    print(_.lexname())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train, dev, test datasets from processed files\n",
    "# check the 'data_loader.ipynb' for details\n",
    "def read_file():\n",
    "    \n",
    "    train_X = []\n",
    "    train_X_num = 0\n",
    "    train_Y = []\n",
    "    train_Y_num = 0\n",
    "    test_X = []\n",
    "    test_X_num = 0\n",
    "    test_Y = []\n",
    "    test_Y_num = 0\n",
    "    dev_X = []\n",
    "    dev_X_num = 0\n",
    "    dev_Y = []\n",
    "    dev_Y_num = 0\n",
    "    \n",
    "    train_word_idx = []\n",
    "    test_word_idx = []\n",
    "    dev_word_idx = []\n",
    "    \n",
    "    # read in csv\n",
    "    with open('data/train_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            train_X.append(row)\n",
    "            train_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {train_X_num} data points for train_X.')\n",
    "\n",
    "    with open('data/train_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            train_Y.append(row)\n",
    "            train_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {train_Y_num} data points for train_Y.')\n",
    "        \n",
    "    with open('data/train_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            train_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(train_word_idx)} data points for train_word_idx.')\n",
    "\n",
    "    with open('data/dev_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            dev_X.append(row)\n",
    "            dev_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {dev_X_num} data points for dev_X.')\n",
    "\n",
    "    with open('data/dev_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            dev_Y.append(row)\n",
    "            dev_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {dev_Y_num} data points for dev_Y.')\n",
    "        \n",
    "    with open('data/dev_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            dev_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(dev_word_idx)} data points for dev_word_idx.')\n",
    "        \n",
    "    with open('data/test_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            test_X.append(row)\n",
    "            test_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {test_X_num} data points for test_X.')\n",
    "\n",
    "    with open('data/test_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            test_Y.append(row)\n",
    "            test_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {test_Y_num} data points for test_Y.')\n",
    "        \n",
    "    with open('data/test_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            test_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(test_word_idx)} data points for test_word_idx.')    \n",
    "        \n",
    "    return train_X, train_Y, test_X, test_Y, dev_X, dev_Y, train_word_idx, test_word_idx, dev_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 67398 data points for train_X.\n",
      "Parsed 67398 data points for train_Y.\n",
      "Parsed 67398 data points for train_word_idx.\n",
      "Parsed 7333 data points for dev_X.\n",
      "Parsed 7333 data points for dev_Y.\n",
      "Parsed 7333 data points for dev_word_idx.\n",
      "Parsed 7123 data points for test_X.\n",
      "Parsed 7123 data points for test_Y.\n",
      "Parsed 7123 data points for test_word_idx.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nword_choice = 'level'\\n\\nnew_train_X = []\\nnew_train_Y = []\\nnew_train_idx = []\\ndistri_train = np.zeros(len(all_test_senses[word_choice]))\\n# stst = 0\\nfor index, sen in enumerate(train_X):\\n    \\n    if sen[train_word_idx[index]] == word_choice:\\n        new_train_idx.append(train_word_idx[index])\\n        new_train_X.append(sen)\\n        new_train_Y.append(train_Y[index])\\n        distri_train += np.asarray(train_Y[index])\\n        # summ = train_Y[index][0] + train_Y[index][1]\\n        # if summ != 2:\\n            # stst += 1\\n# print('stst: {}'.format(stst))        \\nprint('distri of train: {}'.format(distri_train))\\n        \\nnew_test_X = []\\nnew_test_Y = []\\nnew_test_idx = []\\ndistri_test = np.zeros(len(all_test_senses[word_choice]))\\nfor index, sen in enumerate(test_X):\\n    \\n    if sen[test_word_idx[index]] == word_choice:\\n        new_test_idx.append(test_word_idx[index])\\n        new_test_X.append(sen)\\n        new_test_Y.append(test_Y[index])\\n        # print(test_Y[index])\\n        distri_test += np.asarray(test_Y[index])\\nprint('distri of test: {}'.format(distri_test))\\n\\nnew_dev_X = []\\nnew_dev_Y = []\\nnew_dev_idx = []\\ndistri_dev = np.zeros(len(all_test_senses[word_choice]))\\nfor index, sen in enumerate(dev_X):\\n        \\n    if sen[dev_word_idx[index]] == word_choice:\\n        new_dev_idx.append(dev_word_idx[index])\\n        new_dev_X.append(sen)\\n        new_dev_Y.append(dev_Y[index])\\n        distri_dev += np.asarray(dev_Y[index])\\nprint('distri of dev: {}'.format(distri_dev))\\n\\ntarget_senses = all_senses[word_choice]\\nnew_all_senses = {word_choice : target_senses}\\ntarget_def = all_definitions[word_choice]\\nnew_all_def = {word_choice : target_def}\\n\\n# limit the supersense to only the test word\\nnew_all_supersenses = {}\\nfor supersense in all_supersenses.keys():\\n    for tuples in all_supersenses[supersense]:\\n        \\n        if tuples[0] == word_choice:\\n            if new_all_supersenses.get(supersense, 'e') != 'e':\\n                new_all_supersenses[supersense].add((word_choice, tuples[1]))\\n            else:\\n                new_all_supersenses[supersense] = {(word_choice, tuples[1])}\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all the structured data\n",
    "train_X, train_Y, test_X, test_Y, dev_X, dev_Y, train_word_idx, test_word_idx, dev_word_idx = read_file()\n",
    "\n",
    "# test on one word\n",
    "'''\n",
    "word_choice = 'level'\n",
    "\n",
    "new_train_X = []\n",
    "new_train_Y = []\n",
    "new_train_idx = []\n",
    "distri_train = np.zeros(len(all_test_senses[word_choice]))\n",
    "# stst = 0\n",
    "for index, sen in enumerate(train_X):\n",
    "    \n",
    "    if sen[train_word_idx[index]] == word_choice:\n",
    "        new_train_idx.append(train_word_idx[index])\n",
    "        new_train_X.append(sen)\n",
    "        new_train_Y.append(train_Y[index])\n",
    "        distri_train += np.asarray(train_Y[index])\n",
    "        # summ = train_Y[index][0] + train_Y[index][1]\n",
    "        # if summ != 2:\n",
    "            # stst += 1\n",
    "# print('stst: {}'.format(stst))        \n",
    "print('distri of train: {}'.format(distri_train))\n",
    "        \n",
    "new_test_X = []\n",
    "new_test_Y = []\n",
    "new_test_idx = []\n",
    "distri_test = np.zeros(len(all_test_senses[word_choice]))\n",
    "for index, sen in enumerate(test_X):\n",
    "    \n",
    "    if sen[test_word_idx[index]] == word_choice:\n",
    "        new_test_idx.append(test_word_idx[index])\n",
    "        new_test_X.append(sen)\n",
    "        new_test_Y.append(test_Y[index])\n",
    "        # print(test_Y[index])\n",
    "        distri_test += np.asarray(test_Y[index])\n",
    "print('distri of test: {}'.format(distri_test))\n",
    "\n",
    "new_dev_X = []\n",
    "new_dev_Y = []\n",
    "new_dev_idx = []\n",
    "distri_dev = np.zeros(len(all_test_senses[word_choice]))\n",
    "for index, sen in enumerate(dev_X):\n",
    "        \n",
    "    if sen[dev_word_idx[index]] == word_choice:\n",
    "        new_dev_idx.append(dev_word_idx[index])\n",
    "        new_dev_X.append(sen)\n",
    "        new_dev_Y.append(dev_Y[index])\n",
    "        distri_dev += np.asarray(dev_Y[index])\n",
    "print('distri of dev: {}'.format(distri_dev))\n",
    "\n",
    "target_senses = all_senses[word_choice]\n",
    "new_all_senses = {word_choice : target_senses}\n",
    "target_def = all_definitions[word_choice]\n",
    "new_all_def = {word_choice : target_def}\n",
    "\n",
    "# limit the supersense to only the test word\n",
    "new_all_supersenses = {}\n",
    "for supersense in all_supersenses.keys():\n",
    "    for tuples in all_supersenses[supersense]:\n",
    "        \n",
    "        if tuples[0] == word_choice:\n",
    "            if new_all_supersenses.get(supersense, 'e') != 'e':\n",
    "                new_all_supersenses[supersense].add((word_choice, tuples[1]))\n",
    "            else:\n",
    "                new_all_supersenses[supersense] = {(word_choice, tuples[1])}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from trainer import *\n",
    "\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "epochs = 0\n",
    "\n",
    "# test on one word\n",
    "trainer = Trainer(epochs = epochs, elmo_class = elmo, all_senses = all_senses, all_supersenses = all_supersenses)\n",
    "# trainer = Trainer(epochs = epochs, elmo_class = elmo, all_senses = new_all_senses, all_supersenses = new_all_supersenses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'half'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-dd49a13fbee1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_rs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_word_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_word_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# small test on only one word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# train_losses, dev_losses, dev_rs = trainer.train(new_train_X, new_train_Y, new_train_idx, new_dev_X, new_dev_Y, new_dev_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WordSense/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_X, train_Y, train_idx, dev_X, dev_Y, dev_idx, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdev_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdev_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_Y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_trainer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m                 \u001b[0;31m# old = self._model.definition_embeddings['spring'].clone().detach()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WordSense/trainer.py\u001b[0m in \u001b[0;36m_initialize_trainer_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_initialize_trainer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;31m# print(len(self.all_senses.keys()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_senses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_senses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melmo_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melmo_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_supersenses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_supersenses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WordSense/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, all_senses, all_supersenses, output_size, embedding_size, elmo_class, tuned_embed_size, mlp_dropout, lstm_hidden_size, MLP_sizes, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;31m# initiate all the supersense embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0msuper_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_supersense_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msupersense_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameterDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/WordSense/model.py\u001b[0m in \u001b[0;36m_init_supersense_embeddings\u001b[0;34m(self, output_size)\u001b[0m\n\u001b[1;32m    100\u001b[0m                                 \u001b[0mword_lemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                                 \u001b[0mword_sense\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                                 \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_senses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_lemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_sense\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                                 \u001b[0;31m# print(self.definition_embeddings[word_lemma][:, index].size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                                 \u001b[0msupersense_vec\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition_embeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_lemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'half'"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "train_losses, dev_losses, dev_rs = trainer.train(train_X, train_Y, train_word_idx, dev_X, dev_Y, dev_word_idx)\n",
    "\n",
    "# small test on only one word\n",
    "# train_losses, dev_losses, dev_rs = trainer.train(new_train_X, new_train_Y, new_train_idx, new_dev_X, new_dev_Y, new_dev_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "with open('train_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(train_losses)\n",
    "\n",
    "    \n",
    "with open('dev_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(dev_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "# rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(train_losses, ms = 4, marker = 's', label = \"Train Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(new_train_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('train_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "# rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(dev_losses, ms = 4, marker = 'o', label = \"Dev Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(new_dev_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('dev_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "# modify to test only one word for now\n",
    "cos = nn.CosineSimilarity(dim = 1, eps = 1e-6).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "correct_count = 0\n",
    "known_test_size = 0\n",
    "unknown_test_size = 0\n",
    "unknown_correct_count = 0\n",
    "\n",
    "embds = []\n",
    "\n",
    "# overall accuracy\n",
    "for test_idx, test_sen in enumerate(test_X):\n",
    "    \n",
    "    test_lemma = test_sen[test_word_idx[test_idx]]\n",
    "    # print(test_sen)\n",
    "    test_emb = trainer._model.forward(test_sen, test_word_idx[test_idx]).view(1, -1).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    # print(test_emb)\n",
    "    all_similarity = []\n",
    "    embds.append(test_emb)\n",
    "    \n",
    "    # if it is a new word\n",
    "    # only test on the supersense\n",
    "    if all_senses.get(test_lemma, 'e') == 'e':\n",
    "        \n",
    "        unknown_test_size += 1\n",
    "        test_result = ''\n",
    "        best_sim = -float('inf')\n",
    "        \n",
    "        for n, new_s in enumerate(all_test_senses[test_lemma]):\n",
    "            \n",
    "            new_super = wn.synset(new_s).lexname().replace('.', '_')\n",
    "            super_vec = trainer._model.supersense_embeddings[new_super].view(1, -1).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            cos_sim = cos(test_emb, super_vec)\n",
    "            \n",
    "            if cos_sim > best_sim:\n",
    "                test_result = new_super\n",
    "                best_sim = cos_sim\n",
    "                \n",
    "        correct_super = []\n",
    "        for q, respon in test_Y[test_idx]:\n",
    "            if respon:\n",
    "                correct_s = wn.synset(all_test_senses[test_lemma][q]).lexname().replace('.', '_')\n",
    "                correct_super.append(correct_s)            \n",
    "        if test_result in correct_super:\n",
    "            unknown_correct_count += 1\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # if it is a known word\n",
    "        known_test_size += 1\n",
    "        \n",
    "        for k, sense in enumerate(all_senses[test_lemma]):\n",
    "            definition_vec = trainer._model.definition_embeddings[test_lemma][:, k].view(1, -1).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "            cos_sim = cos(test_emb, definition_vec)\n",
    "            all_similarity.append(cos_sim)\n",
    "        # print(all_similarity)\n",
    "        test_result = all_similarity.index(max(all_similarity))\n",
    "        # print(\"result index: {}\".format(test_result))\n",
    "        if test_Y[test_idx][test_result] == 1:\n",
    "            correct_count += 1\n",
    "\n",
    "print('test size for known words: {}'.format(known_test_size))\n",
    "print('accuracy for known words: {}'.format(correct_count / known_test_size))\n",
    "\n",
    "print('test size for unknown words: {}'.format(unknown_test_size))\n",
    "print('accuracy for unknown words: {}'.format(unknown_correct_count / unknown_test_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug\n",
    "\n",
    "print(embds[0])\n",
    "print(embds[1])\n",
    "\n",
    "for q in range(len(embds)):\n",
    "    for p in range(len(embds)):\n",
    "        \n",
    "        print(\"q: {}, p: {}\\ncos: {}\".format(q, p, cos(embds[q], embds[p])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
