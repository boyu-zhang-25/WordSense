{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import string\n",
    "import itertools\n",
    "from io import open\n",
    "from conllu import parse_incr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ViewBackward object at 0x10cbc6e48>\n",
      "True\n",
      "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [2.9802e-08, 0.0000e+00, 0.0000e+00]])\n",
      "tensor([[1.0000, 1.0000, 1.0000],\n",
      "        [1.0000, 1.0000, 1.0000],\n",
      "        [0.9999, 1.0000, 1.0000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# tests for pytorch computation graph\n",
    "from torch.nn import CosineEmbeddingLoss\n",
    "l = CosineEmbeddingLoss()\n",
    "import torch.optim as optim\n",
    "\n",
    "sense_vec = torch.ones(3, 1)\n",
    "sense_vec.requires_grad = True\n",
    "def_vec = torch.randn(3, 1)\n",
    "def_vec.requires_grad = True\n",
    "\n",
    "matrix = torch.ones(3, 3)\n",
    "matrix.requires_grad = True\n",
    "vec = matrix[2, :].view(3, 1)\n",
    "print(vec.grad_fn)\n",
    "print(vec.requires_grad)\n",
    "label = torch.randn(3, 1)\n",
    "\n",
    "opt = optim.Adam([matrix])\n",
    "test_vec = torch.ones(def_vec.size())\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    opt.zero_grad()\n",
    "    loss = l(vec, label, test_vec)\n",
    "    # print(loss)\n",
    "    loss.backward()\n",
    "    print(matrix.grad)\n",
    "    # print(def_vec.grad)\n",
    "    \n",
    "    opt.step()\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the WSD dataset first\n",
    "\n",
    "'''\n",
    "Copyright@\n",
    "White, A. S., D. Reisinger, K. Sakaguchi, T. Vieira, S. Zhang, R. Rudinger, K. Rawlins, & B. Van Durme. 2016. \n",
    "[Universal decompositional semantics on universal dependencies]\n",
    "(http://aswhite.net/media/papers/white_universal_2016.pdf). \n",
    "To appear in *Proceedings of the Conference on Empirical Methods in Natural Language Processing 2016*.\n",
    "'''\n",
    "\n",
    "def parse_wsd_data():\n",
    "\n",
    "    # parse the WSD dataset\n",
    "    wsd_data = []\n",
    "\n",
    "    # read in tsv by White et. al., 2016\n",
    "    with open('data/wsd/wsd_eng_ud1.2_10262016.tsv', mode = 'r') as wsd_file:\n",
    "\n",
    "        tsv_reader = csv.DictReader(wsd_file, delimiter = '\\t')      \n",
    "\n",
    "        # store the data: ordered dict row\n",
    "        for row in tsv_reader:                                \n",
    "\n",
    "            # each data vector\n",
    "            wsd_data.append(row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print('Parsed {} word sense data from White et. al., 2016.'.format(len(wsd_data)))\n",
    "\n",
    "    return wsd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 439312 word sense data from White et. al., 2016.\n"
     ]
    }
   ],
   "source": [
    "# get the raw wsd data\n",
    "wsd_data = parse_wsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return: \n",
    "all senses for each word \n",
    "all definitions for each word\n",
    "all supersenses\n",
    "from the EUD for train, test, and dev dataset\n",
    "index provided by WSD dataset by White et. al.\n",
    "'''\n",
    "# get all the senses and definitions for each word from WSD dataset\n",
    "# order of senses and definitions are in order\n",
    "def get_all_senses_and_definitions(wsd_data):\n",
    "\n",
    "    # all senses for each \n",
    "    all_senses = {}\n",
    "    all_definitions = {}\n",
    "    all_supersenses = {}\n",
    "    \n",
    "    # for test purpose: only load specific amount of data\n",
    "    for i in range(len(wsd_data)):\n",
    "\n",
    "        # get the original sentence from EUD\n",
    "        sentence_id = wsd_data[i].get('Sentence.ID')\n",
    "        \n",
    "        # get the definitions for the target word from EUD\n",
    "        definition = wsd_data[i].get('Sense.Definition').split(' ')\n",
    "        \n",
    "        # the index in EUD is 1-based!!!\n",
    "        sentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "        word_index = int(wsd_data[i].get('Arg.Token')) - 1\n",
    "        word_lemma = wsd_data[i].get('Arg.Lemma')\n",
    "        word_sense = wsd_data[i].get('Synset')\n",
    "        response = wsd_data[i].get('Sense.Response')\n",
    "        \n",
    "        # supersense-> (word_lemma, word_sense) dictionary\n",
    "        super_s = wn.synset(word_sense).lexname().replace('.', '_')\n",
    "        if all_supersenses.get(super_s, 'not_exist') != 'not_exist':\n",
    "            all_supersenses[super_s].add((word_lemma, word_sense))\n",
    "        else:\n",
    "            all_supersenses[super_s] = {(word_lemma, word_sense)}\n",
    "\n",
    "        # if the word already exits: add the new sense to the list\n",
    "        # else: creata a new list for the word\n",
    "        if all_senses.get(word_lemma, 'not_exist') != 'not_exist':\n",
    "            if word_sense not in all_senses[word_lemma]:\n",
    "                all_senses[word_lemma].append(word_sense)\n",
    "        else:\n",
    "            all_senses[word_lemma] = []\n",
    "            all_senses[word_lemma].append(word_sense)            \n",
    "            \n",
    "        if all_definitions.get(word_lemma,'not_exist') != 'not_exist':\n",
    "            if definition not in all_definitions[word_lemma]: \n",
    "                all_definitions[word_lemma].append(definition)\n",
    "        else:\n",
    "            all_definitions[word_lemma] = []\n",
    "            all_definitions[word_lemma].append(definition)\n",
    "        \n",
    "    return all_senses, all_definitions, all_supersenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the senses and definitions\n",
    "all_senses, all_definitions, all_supersenses = get_all_senses_and_definitions(wsd_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe specific Synset method is lexname, e.g. wn.synsets('spring')[0].lexname(). \\nThat should make it really easy to get the suspersenses.\\nAnd if you have the synset name–e.g. 'spring.n.01'\\nyou can access the supersense directly: wn.synset('spring.n.01').lexname().\\nWhich returns 'noun.time'.\\nAnd wn.synset('spring.n.02').lexname() returns 'noun.artifact'\\n\\nfor idx, d in enumerate(all_definitions['spring']):\\n    print(d)\\n    print(wn.synset(all_senses['spring'][idx]).lexname())\\n\\nfor _ in wn.synsets('spring'):\\n    print(_.lexname())\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for the WordNet NLTK API\n",
    "'''\n",
    "The specific Synset method is lexname, e.g. wn.synsets('spring')[0].lexname(). \n",
    "That should make it really easy to get the suspersenses.\n",
    "And if you have the synset name–e.g. 'spring.n.01'\n",
    "you can access the supersense directly: wn.synset('spring.n.01').lexname().\n",
    "Which returns 'noun.time'.\n",
    "And wn.synset('spring.n.02').lexname() returns 'noun.artifact'\n",
    "\n",
    "for idx, d in enumerate(all_definitions['spring']):\n",
    "    print(d)\n",
    "    print(wn.synset(all_senses['spring'][idx]).lexname())\n",
    "\n",
    "for _ in wn.synsets('spring'):\n",
    "    print(_.lexname())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train, dev, test datasets from processed files\n",
    "# check the 'data_loader.ipynb' for details\n",
    "def read_file():\n",
    "    \n",
    "    train_X = []\n",
    "    train_X_num = 0\n",
    "    train_Y = []\n",
    "    train_Y_num = 0\n",
    "    test_X = []\n",
    "    test_X_num = 0\n",
    "    test_Y = []\n",
    "    test_Y_num = 0\n",
    "    dev_X = []\n",
    "    dev_X_num = 0\n",
    "    dev_Y = []\n",
    "    dev_Y_num = 0\n",
    "    \n",
    "    train_word_idx = []\n",
    "    test_word_idx = []\n",
    "    dev_word_idx = []\n",
    "    \n",
    "    # read in csv\n",
    "    with open('data/train_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            train_X.append(row)\n",
    "            train_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {train_X_num} data points for train_X.')\n",
    "\n",
    "    with open('data/train_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            train_Y.append(row)\n",
    "            train_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {train_Y_num} data points for train_Y.')\n",
    "        \n",
    "    with open('data/train_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            train_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(train_word_idx)} data points for train_word_idx.')\n",
    "\n",
    "    with open('data/dev_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            dev_X.append(row)\n",
    "            dev_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {dev_X_num} data points for dev_X.')\n",
    "\n",
    "    with open('data/dev_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            dev_Y.append(row)\n",
    "            dev_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {dev_Y_num} data points for dev_Y.')\n",
    "        \n",
    "    with open('data/dev_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            dev_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(dev_word_idx)} data points for dev_word_idx.')\n",
    "        \n",
    "    with open('data/test_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            test_X.append(row)\n",
    "            test_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {test_X_num} data points for test_X.')\n",
    "\n",
    "    with open('data/test_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            test_Y.append(row)\n",
    "            test_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {test_Y_num} data points for test_Y.')\n",
    "        \n",
    "    with open('data/test_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            test_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(test_word_idx)} data points for test_word_idx.')    \n",
    "        \n",
    "    return train_X, train_Y, test_X, test_Y, dev_X, dev_Y, train_word_idx, test_word_idx, dev_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 67398 data points for train_X.\n",
      "Parsed 67398 data points for train_Y.\n",
      "Parsed 67398 data points for train_word_idx.\n",
      "Parsed 7333 data points for dev_X.\n",
      "Parsed 7333 data points for dev_Y.\n",
      "Parsed 7333 data points for dev_word_idx.\n",
      "Parsed 7123 data points for test_X.\n",
      "Parsed 7123 data points for test_Y.\n",
      "Parsed 7123 data points for test_word_idx.\n",
      "distri of train: [35, 43, 27, 11, 6, 3, 7, 5]\n",
      "distri of test: [5, 5, 4, 1, 1, 1, 2, 1]\n",
      "distri of dev: [4, 3, 0, 2, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "train_X, train_Y, test_X, test_Y, dev_X, dev_Y, train_word_idx, test_word_idx, dev_word_idx = read_file()\n",
    "\n",
    "# test on one word\n",
    "word_choice = 'level'\n",
    "\n",
    "new_train_X = []\n",
    "new_train_Y = []\n",
    "new_train_idx = []\n",
    "distri_train = [0 for _ in range(len(all_senses[word_choice]))]\n",
    "for index, sen in enumerate(train_X):\n",
    "    \n",
    "    if sen[train_word_idx[index]] == word_choice:\n",
    "        new_train_idx.append(train_word_idx[index])\n",
    "        new_train_X.append(sen)\n",
    "        new_train_Y.append(train_Y[index])\n",
    "        for i, response in enumerate(train_Y[index]):\n",
    "            if response:\n",
    "                distri_train[i] += 1\n",
    "print('distri of train: {}'.format(distri_train))\n",
    "        \n",
    "new_test_X = []\n",
    "new_test_Y = []\n",
    "new_test_idx = []\n",
    "distri_test = [0 for _ in range(len(all_senses[word_choice]))]\n",
    "for index, sen in enumerate(test_X):\n",
    "    \n",
    "    if sen[test_word_idx[index]] == word_choice:\n",
    "        new_test_idx.append(test_word_idx[index])\n",
    "        new_test_X.append(sen)\n",
    "        new_test_Y.append(test_Y[index])\n",
    "        for i, response in enumerate(test_Y[index]):\n",
    "            if response:\n",
    "                distri_test[i] += 1\n",
    "print('distri of test: {}'.format(distri_test))\n",
    "\n",
    "new_dev_X = []\n",
    "new_dev_Y = []\n",
    "new_dev_idx = []\n",
    "distri_dev = [0 for _ in range(len(all_senses[word_choice]))]\n",
    "for index, sen in enumerate(dev_X):\n",
    "        \n",
    "    if sen[dev_word_idx[index]] == word_choice:\n",
    "        new_dev_idx.append(dev_word_idx[index])\n",
    "        new_dev_X.append(sen)\n",
    "        new_dev_Y.append(dev_Y[index])\n",
    "        for i, response in enumerate(dev_Y[index]):\n",
    "            if response:\n",
    "                distri_dev[i] += 1\n",
    "print('distri of dev: {}'.format(distri_dev))\n",
    "\n",
    "target_senses = all_senses[word_choice]\n",
    "new_all_senses = {word_choice : target_senses}\n",
    "# print(new_all_senses)\n",
    "target_def = all_definitions[word_choice]\n",
    "new_all_def = {word_choice : target_def}\n",
    "\n",
    "new_all_supersenses = {}\n",
    "for supersense in all_supersenses.keys():\n",
    "    for tuples in all_supersenses[supersense]:\n",
    "        \n",
    "        if tuples[0] == word_choice:\n",
    "            if new_all_supersenses.get(supersense, 'e') != 'e':\n",
    "                new_all_supersenses[supersense].add((word_choice, tuples[1]))\n",
    "            else:\n",
    "                new_all_supersenses[supersense] = {(word_choice, tuples[1])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom torchviz import make_dot\\n\\nmodel = Model(all_senses = all_senses, elmo_class = elmo)\\nsense_embedding = model.forward(train_X[0], train_word_idx[0])\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import *\n",
    "from trainer import *\n",
    "\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()\n",
    "\n",
    "'''\n",
    "from torchviz import make_dot\n",
    "\n",
    "model = Model(all_senses = all_senses, elmo_class = elmo)\n",
    "sense_embedding = model.forward(train_X[0], train_word_idx[0])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "epochs = 30\n",
    "trainer = Trainer(epochs = epochs, elmo_class = elmo, all_senses = new_all_senses, all_supersenses = new_all_supersenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############   Model Parameters   ##############\n",
      "layers.word_sense.0.weight torch.Size([512, 512])\n",
      "layers.word_sense.0.bias torch.Size([512])\n",
      "layers.word_sense.2.weight torch.Size([300, 512])\n",
      "layers.word_sense.2.bias torch.Size([300])\n",
      "dimension_reduction_MLP.weight torch.Size([256, 3072])\n",
      "dimension_reduction_MLP.bias torch.Size([256])\n",
      "wsd_lstm.weight_ih_l0 torch.Size([1024, 256])\n",
      "wsd_lstm.weight_hh_l0 torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l0 torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l0 torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l0_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.weight_hh_l0_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l0_reverse torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l0_reverse torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l1 torch.Size([1024, 512])\n",
      "wsd_lstm.weight_hh_l1 torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l1 torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l1 torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l1_reverse torch.Size([1024, 512])\n",
      "wsd_lstm.weight_hh_l1_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l1_reverse torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l1_reverse torch.Size([1024])\n",
      "definition_embeddings.level torch.Size([300, 8])\n",
      "supersense_embeddings.noun_artifact torch.Size([300, 1])\n",
      "supersense_embeddings.noun_attribute torch.Size([300, 1])\n",
      "supersense_embeddings.noun_cognition torch.Size([300, 1])\n",
      "supersense_embeddings.noun_state torch.Size([300, 1])\n",
      "##################################################\n",
      "[Epoch: 1/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752040eacec6478e929de314192229ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Mean Training Loss: 9.431071445859711\n",
      "Epoch: 1, Mean Dev Loss: 8.965237753731865\n",
      "[Epoch: 2/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cc86371e53a4c75bb5e7f165c52f934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Mean Training Loss: 9.348735376336109\n",
      "Epoch: 2, Mean Dev Loss: 8.909522601536342\n",
      "[Epoch: 3/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0066e10d833d43a8889407ea64bfbabd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Mean Training Loss: 9.296282664112661\n",
      "Epoch: 3, Mean Dev Loss: 8.879523277282715\n",
      "[Epoch: 4/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "344144183e5041fa9144639a20da3de2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Mean Training Loss: 9.288850170442428\n",
      "Epoch: 4, Mean Dev Loss: 8.922855922154017\n",
      "[Epoch: 5/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dbad6b993e4cc1934a1b00b96219ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Mean Training Loss: 9.296436671552987\n",
      "Epoch: 5, Mean Dev Loss: 8.889524459838867\n",
      "[Epoch: 6/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db0e830e8ab74d62b22e429c8d20a731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Mean Training Loss: 9.27727960718089\n",
      "Epoch: 6, Mean Dev Loss: 8.824761254446846\n",
      "[Epoch: 7/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017fa3ce80924960b6c1389f1ef76b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Mean Training Loss: 9.224788589039068\n",
      "Epoch: 7, Mean Dev Loss: 8.780475752694267\n",
      "[Epoch: 8/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a9c0d358844fc6acb67617a985da00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Mean Training Loss: 9.232374827067057\n",
      "Epoch: 8, Mean Dev Loss: 8.800951957702637\n",
      "[Epoch: 9/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d706e73c504490b858d7f98df32bd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Mean Training Loss: 9.244482067809708\n",
      "Epoch: 9, Mean Dev Loss: 8.863808086940221\n",
      "[Epoch: 10/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac94f0cb94b24cdf945b41cfb30d53c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Mean Training Loss: 9.239462940172217\n",
      "Epoch: 10, Mean Dev Loss: 8.846665791102819\n",
      "[Epoch: 11/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e80fabd062f486b8a2f929ace10dae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Mean Training Loss: 9.219424653327328\n",
      "Epoch: 11, Mean Dev Loss: 8.85999870300293\n",
      "[Epoch: 12/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6047d8769dbc4cf0ac04a20c869eb65b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Mean Training Loss: 9.219156429685395\n",
      "Epoch: 12, Mean Dev Loss: 8.822380883353096\n",
      "[Epoch: 13/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f29276e5ead42d49086359438fe0139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Mean Training Loss: 9.191915369581903\n",
      "Epoch: 13, Mean Dev Loss: 8.789046696254186\n",
      "[Epoch: 14/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e598596b81c74c01ba94b5b060a37163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Mean Training Loss: 9.15007634546565\n",
      "Epoch: 14, Mean Dev Loss: 8.773333549499512\n",
      "[Epoch: 15/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa643c59aaa44c2098df1a48b42f8356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Mean Training Loss: 9.162144682873254\n",
      "Epoch: 15, Mean Dev Loss: 8.801903315952845\n",
      "[Epoch: 16/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2113f2fec5c485aa4cf53ea7af16eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Mean Training Loss: 9.170152231194507\n",
      "Epoch: 16, Mean Dev Loss: 8.859998839242119\n",
      "[Epoch: 17/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61ed14f1b874f7095f7747ca732ecce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Mean Training Loss: 9.185746719097269\n",
      "Epoch: 17, Mean Dev Loss: 8.852856772286552\n",
      "[Epoch: 18/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f18f7c42db943f38f929d836d6539ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Mean Training Loss: 9.165248498149301\n",
      "Epoch: 18, Mean Dev Loss: 8.81809411730085\n",
      "[Epoch: 19/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c08fbed7bf4956abd408c59a323dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Mean Training Loss: 9.150076033055097\n",
      "Epoch: 19, Mean Dev Loss: 8.835713931492396\n",
      "[Epoch: 20/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3c60f904b44793b4eeec083e91a13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Mean Training Loss: 9.154826926088882\n",
      "Epoch: 20, Mean Dev Loss: 8.837618691580635\n",
      "[Epoch: 21/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1851edf58f04736bde78e817abcd958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21, Mean Training Loss: 9.131762230533292\n",
      "Epoch: 21, Mean Dev Loss: 8.809999738420759\n",
      "[Epoch: 22/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f7e3ced8f934669968cc69435add41e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22, Mean Training Loss: 9.130152910605243\n",
      "Epoch: 22, Mean Dev Loss: 8.793808800833565\n",
      "[Epoch: 23/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f2c767796841c0975806035eee0935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23, Mean Training Loss: 9.11911774777818\n",
      "Epoch: 23, Mean Dev Loss: 8.801427568708148\n",
      "[Epoch: 24/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee7912e8298949898c9d10c34b71bd4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24, Mean Training Loss: 9.110803911055642\n",
      "Epoch: 24, Mean Dev Loss: 8.784761428833008\n",
      "[Epoch: 25/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff8d194268e4597b35a2bcaa524ee63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Mean Training Loss: 9.101838473615976\n",
      "Epoch: 25, Mean Dev Loss: 8.784761428833008\n",
      "[Epoch: 26/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b17e675de17d42bbb477c599a407b87a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26, Mean Training Loss: 9.120344123621097\n",
      "Epoch: 26, Mean Dev Loss: 8.814284869602748\n",
      "[Epoch: 27/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b057ca5f24584fe181e457ec70dfb758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27, Mean Training Loss: 9.140803786529892\n",
      "Epoch: 27, Mean Dev Loss: 8.826189722333636\n",
      "[Epoch: 28/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb6764a0c364fa880da3c2e21987c1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28, Mean Training Loss: 9.132987987035992\n",
      "Epoch: 28, Mean Dev Loss: 8.847142628261022\n",
      "[Epoch: 29/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96b9aa43cd7a42f5b97769f373269845",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29, Mean Training Loss: 9.136628063245752\n",
      "Epoch: 29, Mean Dev Loss: 8.838095256260463\n",
      "[Epoch: 30/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8a70fdb380b423f9e38173d2a1427af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Mean Training Loss: 9.130804500360599\n",
      "Epoch: 30, Mean Dev Loss: 8.818094662257604\n"
     ]
    }
   ],
   "source": [
    "# get the results\n",
    "# train_losses, dev_losses, dev_rs = trainer.train(train_X, train_Y, train_word_idx, dev_X, dev_Y, dev_word_idx, development = False)\n",
    "# small test\n",
    "train_losses, dev_losses, dev_rs = trainer.train(new_train_X, new_train_Y, new_train_idx, new_dev_X, new_dev_Y, new_dev_idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "with open('train_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(train_losses)\n",
    "\n",
    "    \n",
    "with open('dev_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(dev_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(train_losses, ms = 4, marker = 's', label = \"Train Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(new_train_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('train_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(dev_losses, ms = 4, marker = 'o', label = \"Dev Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(new_dev_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('dev_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test size: 9\n",
      "overall accuracy: 0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "# modify the test set smaller for now\n",
    "cos = nn.CosineSimilarity(dim = 0, eps = 1e-6)\n",
    "correct_count = 0\n",
    "        \n",
    "# overall accuracy\n",
    "for test_idx, test_sen in enumerate(new_test_X):\n",
    "    \n",
    "    test_lemma = test_sen[new_test_idx[test_idx]]\n",
    "    test_emb = trainer._model.forward(test_sen, new_test_idx[test_idx])\n",
    "    all_similarity = []\n",
    "    \n",
    "    for k, sense in enumerate(new_all_senses[test_lemma]):\n",
    "        definition_vec = trainer._model.definition_embeddings[test_lemma][:, k].view(trainer._model.output_size, -1)\n",
    "        cos_sim = cos(test_emb, definition_vec)\n",
    "        all_similarity.append(cos_sim)\n",
    "    \n",
    "    test_result = all_similarity.index(max(all_similarity))\n",
    "    if new_test_Y[test_idx][test_result] == 1:\n",
    "        correct_count += 1\n",
    "\n",
    "print('test size: {}'.format(len(new_test_X)))\n",
    "print('overall accuracy: {}'.format(correct_count / len(new_test_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
