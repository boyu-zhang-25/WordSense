{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import itertools\n",
    "from io import open\n",
    "from conllu import parse_incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the structure of the fine-tuning MLP for WSD\n",
    "def print_fine_tuning_MLP(model, param):\n",
    "\n",
    "\tprint('\\n******************* fine-tuning MLP structure ***********************')\n",
    "\n",
    "\tprint('Current Task: {}'.format(param))\n",
    "\tmodule_dict = model.layers[param]\n",
    "\n",
    "\tfor module in module_dict:\n",
    "\t\tprint(module)\n",
    "\n",
    "\tprint('**********************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the whole model \n",
    "def print_whole_model(model):\n",
    "\n",
    "\tprint('\\nAll parameters in the model:')\n",
    "\tfor name, param in model.named_parameters():\n",
    "\t\tif param.requires_grad:\n",
    "\t\t\tprint(name, param.size())\n",
    "\n",
    "\tprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return: \n",
    "all sentences\n",
    "target word index\n",
    "target word lemma\n",
    "target word sense\n",
    "all senses for each word \n",
    "from the EUD for train, test, and dev dataset\n",
    "index provided by WSD dataset by White et. al.\n",
    "'''\n",
    "\n",
    "def get_sentences_and_senses(wsd_data, train_data, test_data, dev_data, sen_num):\n",
    "    \n",
    "\t# get the sentences, target word index, annotator response, lemma, and target word sense\n",
    "\ttrain_sentences, train_word_index, train_word_sense = ([] for i in range(3))\n",
    "\ttest_sentences, test_word_index, test_word_sense = ([] for i in range(3))\n",
    "\tdev_sentences, dev_word_index, dev_word_sense = ([] for i in range(3))\n",
    "\ttrain_lemma, test_lemma, dev_lemma = ([] for i in range(3))\n",
    "\ttrain_response, test_response, dev_response = ([] for i in range(3))\n",
    "    \n",
    "    # all senses for each \n",
    "\tall_senses = {}\n",
    "    \n",
    "\t# for test purpose: only load specific amount of data\n",
    "\tfor i in range(sen_num):\n",
    "\n",
    "\t\t# get the original sentence from EUD\n",
    "\t\tsentence_id = wsd_data[i].get('Sentence.ID')\n",
    "\n",
    "\t\t# the index in EUD is 1-based!!!\n",
    "\t\tsentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "\t\t# print('sentence id {} i {}'.format(sentence_id, i))\n",
    "\t\tword_index = int(wsd_data[i].get('Arg.Token')) - 1\n",
    "\t\tword_lemma = wsd_data[i].get('Arg.Lemma')\n",
    "\t\tword_sense = wsd_data[i].get('Synset')\n",
    "\t\tresponse = wsd_data[i].get('Sense.Response')\n",
    "\n",
    "\t\tif \"train\" in sentence_id: \n",
    "\t\t\tsentence = train_data[sentence_number]\n",
    "\n",
    "\t\t\t# the clean sentence in list\n",
    "\t\t\tclean_sentence = [word_dict.get('lemma') for word_dict in sentence]\n",
    "\t\t\ttrain_sentences.append(clean_sentence)\n",
    "\t\t\ttrain_word_index.append(word_index)\n",
    "\t\t\ttrain_word_sense.append(word_sense)\n",
    "\t\t\ttrain_response.append(response)\n",
    "\t\t\ttrain_lemma.append(word_lemma)\n",
    "            \n",
    "\t\telif \"test\" in sentence_id:\n",
    "\t\t\tsentence = test_data[sentence_number]\n",
    "\n",
    "\t\t\t# the clean sentence in list\n",
    "\t\t\tclean_sentence = [word_dict.get('lemma') for word_dict in sentence]\n",
    "\t\t\ttest_sentences.append(clean_sentence)\n",
    "\t\t\ttest_word_index.append(word_index)\n",
    "\t\t\ttest_word_sense.append(word_sense)\n",
    "\t\t\ttest_response.append(response)\n",
    "\t\t\ttest_lemma.append(word_lemma)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tsentence = dev_data[sentence_number]\n",
    "            \n",
    "\t\t\t# the clean sentence in list\n",
    "\t\t\tclean_sentence = [word_dict.get('lemma') for word_dict in sentence]\n",
    "\t\t\tdev_sentences.append(clean_sentence)\n",
    "\t\t\tdev_word_index.append(word_index)\n",
    "\t\t\tdev_word_sense.append(word_sense)\n",
    "\t\t\tdev_response.append(response)\n",
    "\t\t\tdev_lemma.append(word_lemma)\n",
    "\n",
    "        # if the word already exits: add the new sense to the list\n",
    "        # else: creata a new set for the word\n",
    "\t\tif word_lemma in all_senses.keys():\n",
    "\t\t\tall_senses[word_lemma].add(word_sense)\n",
    "\t\telse:\n",
    "\t\t\tall_senses[word_lemma] = {word_sense}         \n",
    "\n",
    "\tprint('Processed {} sentences for test purpose.'.format(sen_num))\n",
    "\tprint('\\n******************* Data Example ***********************')\n",
    "\tprint('Sentence: {}'.format(train_sentences[0]))\n",
    "\tprint('Target Word Index: {}'.format(train_word_index[0]))\n",
    "\tprint('Target Word Sense (index in WordNet 3.1): {}'.format(train_word_sense[0]))\n",
    "\tprint('Annotator Response: {}'.format(train_response[0]))\n",
    "\tprint('All senses for the target word: {}'.format(all_senses[train_lemma[0]]))\n",
    "\tprint('********************************************************')\n",
    "\n",
    "\treturn train_sentences, train_word_sense, train_word_index, test_sentences, test_word_sense, test_word_index, dev_sentences, dev_word_sense, dev_word_index, train_response, test_response, dev_response, all_senses\n",
    "\t\t\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the WSD dataset first\n",
    "# and retrieve all sentences from the EUD\n",
    "\n",
    "'''\n",
    "Copyright@\n",
    "White, A. S., D. Reisinger, K. Sakaguchi, T. Vieira, S. Zhang, R. Rudinger, K. Rawlins, & B. Van Durme. 2016. \n",
    "[Universal decompositional semantics on universal dependencies]\n",
    "(http://aswhite.net/media/papers/white_universal_2016.pdf). \n",
    "To appear in *Proceedings of the Conference on Empirical Methods in Natural Language Processing 2016*.\n",
    "'''\n",
    "\n",
    "def parse_data():\n",
    "\t\n",
    "\t# parse the WSD dataset\n",
    "\twsd_data = []\n",
    "\n",
    "\t# read in tsv by White et. al., 2016\n",
    "\twith open('data/wsd/wsd_eng_ud1.2_10262016.tsv', mode = 'r') as wsd_file:\n",
    "\n",
    "\t\ttsv_reader = csv.DictReader(wsd_file, delimiter = '\\t')\n",
    "\n",
    "\t\t# store the data\n",
    "\t\tfor row in tsv_reader:\n",
    "\n",
    "\t\t\t# each data vector\n",
    "\t\t\twsd_data.append(row)\n",
    "\n",
    "\t\t# make sure all data are parsed\n",
    "\t\tprint('Parsed {} word sense data from White et. al., 2016.'.format(len(wsd_data)))\n",
    "\n",
    "\t# parse the EUD-EWT conllu files and retrieve the sentences\n",
    "\ttrain_file = open(\"data/UD_English-EWT/en_ewt-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\ttrain_data = list(parse_incr(train_file))\n",
    "\tprint('Parsed {} training data from UD_English-EWT/en_ewt-ud-train.conllu.'.format(len(train_data)))\n",
    "\t# 'spring' as the first example in White et. al., 2016\n",
    "\t# print(train_data[1363])\n",
    "\n",
    "\ttest_file = open(\"data/UD_English-EWT/en_ewt-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\ttest_data = list(parse_incr(test_file))\n",
    "\tprint('Parsed {} testing data from UD_English-EWT/en_ewt-ud-test.conllu.'.format(len(test_data)))\n",
    "\n",
    "\tdev_file = open(\"data/UD_English-EWT/en_ewt-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\tdev_data = list(parse_incr(dev_file))\n",
    "\tprint('Parsed {} dev data from UD_English-EWT/en_ewt-ud-dev.conllu.'.format(len(dev_data)))\n",
    "\n",
    "\treturn wsd_data, train_data, test_data, dev_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 439312 word sense data from White et. al., 2016.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-761907108e2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# parse the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwsd_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# return the raw sentences from the EUD for train, test, and dev\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# test the first 20 sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-ece8581714df>\u001b[0m in \u001b[0;36mparse_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# parse the EUD-EWT conllu files and retrieve the sentences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mtrain_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/UD_English-EWT/en_ewt-ud-train.conllu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_incr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Parsed {} training data from UD_English-EWT/en_ewt-ud-train.conllu.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# 'spring' as the first example in White et. al., 2016\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/conllu/__init__.py\u001b[0m in \u001b[0;36mparse_incr\u001b[0;34m(in_file, fields, field_parsers)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_incr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_parsers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_iter_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mTokenList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mparse_token_and_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_parsers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfield_parsers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mparse_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/conllu/parser.py\u001b[0m in \u001b[0;36mparse_token_and_metadata\u001b[0;34m(data, fields, field_parsers)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvar_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfield_parsers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/conllu/parser.py\u001b[0m in \u001b[0;36mparse_line\u001b[0;34m(line, fields, field_parsers)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfield\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfield_parsers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfield_parsers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mParseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mParseException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed parsing field '{}': \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/conllu/parser.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(line, i)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;34m\"head\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_int_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;34m\"deps\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_paired_list_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;34m\"misc\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mparse_dict_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m }\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/conllu/parser.py\u001b[0m in \u001b[0;36mparse_dict_value\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    140\u001b[0m     return OrderedDict([\n\u001b[1;32m    141\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_nullable_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparse_nullable_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     ])\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/conllu/parser.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    140\u001b[0m     return OrderedDict([\n\u001b[1;32m    141\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_nullable_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"=\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mpart\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mparse_nullable_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     ])\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# parse the data\n",
    "wsd_data, train_data, test_data, dev_data = parse_data()\n",
    "\n",
    "# return the raw sentences from the EUD for train, test, and dev\n",
    "# test the first 20 sentences\n",
    "train_sentences, train_word_sense, train_word_index, test_sentences, test_word_sense, test_word_index, dev_sentences, dev_word_sense, dev_word_index, train_response, test_response, dev_response, all_senses = get_sentences_and_senses(wsd_data, train_data, test_data, dev_data, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELMo setup\n",
    "# ELMo is tuned to lower dimension (256) by MLP in Model\n",
    "elmo = ElmoEmbedder()\n",
    "model = Model(elmo_class = elmo, all_senses = all_senses)\n",
    "\n",
    "# print the model \n",
    "print_whole_model(model)\n",
    "\n",
    "# MLP illustration\n",
    "print_fine_tuning_MLP(model, 'WSD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward propagation\n",
    "# ELMo (1024) -> dimension reduction (256) -> bi-LSTM (512) -> fine-tuning MLP (10)\n",
    "model.forward(train_sentences, train_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
