{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import itertools\n",
    "from io import open\n",
    "from conllu import parse_incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Copyright@\n",
    "White, A. S., D. Reisinger, K. Sakaguchi, T. Vieira, S. Zhang, R. Rudinger, K. Rawlins, & B. Van Durme. 2016. [Universal decompositional semantics on universal dependencies](http://aswhite.net/media/papers/white_universal_2016.pdf). To appear in *Proceedings of the Conference on Empirical Methods in Natural Language Processing 2016*.\n",
    "'''\n",
    "\n",
    "# print the structure of the fine-tuning MLP for illustration\n",
    "def print_fine_tuning_MLP(model, param):\n",
    "\n",
    "\tprint('\\n******************* fine-tuning MLP structure ***********************')\n",
    "\n",
    "\tprint('Current Task: {}'.format(param))\n",
    "\tmodule_dict = model.layers[param]\n",
    "\n",
    "\tfor module in module_dict:\n",
    "\t\tprint(module)\n",
    "\n",
    "\tprint('**********************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_whole_model(model):\n",
    "\n",
    "\tprint('\\nAll parameters in the model:')\n",
    "\tfor name, param in model.named_parameters():\n",
    "\t\tif param.requires_grad:\n",
    "\t\t\tprint(name, param.size())\n",
    "\n",
    "\tprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the raw sentences from the EUD for train, test, and dev\n",
    "def get_raw_sentences(wsd_data, train_data, test_data, dev_data, sen_num):\n",
    "\n",
    "\t# get the raw sentences, target word index, and target word sense\n",
    "\ttrain_sentences = []\n",
    "\ttrain_word_index = []\n",
    "\ttrain_word_sense = []\n",
    "\ttest_sentences = []\n",
    "\ttest_word_index = []\n",
    "\ttest_word_sense = []\n",
    "\tdev_sentences = []\n",
    "\tdev_word_index = []\n",
    "\tdev_word_sense = []\n",
    "\n",
    "\t# for test purpose: only load specific amount of data\n",
    "\tfor i in range(sen_num):\n",
    "\n",
    "\t\t# get the original sentence from EUD\n",
    "\t\tsentence_id = wsd_data[i].get('Sentence.ID')\n",
    "\n",
    "\t\t# the index in EUD is 1-based!!!\n",
    "\t\tsentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "\t\t# print('sentence id {} i {}'.format(sentence_id, i))\n",
    "\t\tword_index = int(wsd_data[i].get('Arg.Token')) - 1\n",
    "\t\tword_lemma = wsd_data[i].get('Arg.Lemma')\n",
    "\t\tword_sense = wsd_data[i].get('Synset')\n",
    "\n",
    "\t\tif \"train\" in sentence_id: \n",
    "\t\t\tsentence = train_data[sentence_number]\n",
    "\t\t\t# print(sentence)\n",
    "\t\t\tassert(sentence[word_index].get('lemma') == word_lemma)\n",
    "\n",
    "\t\t\t# the clean sentence in list\n",
    "\t\t\tclean_sentence = [word_dict.get('lemma') for word_dict in sentence]\n",
    "\t\t\t# print(clean_sentence)\n",
    "\t\t\t# print(len(clean_sentence))\n",
    "\t\t\ttrain_sentences.append(clean_sentence)\n",
    "\t\t\ttrain_word_index.append(word_index)\n",
    "\t\t\ttrain_word_sense.append(word_sense)\n",
    "\n",
    "\t\telif \"test\" in sentence_id:\n",
    "\t\t\tsentence = test_data[sentence_number]\n",
    "\t\t\t# print(sentence)\n",
    "\t\t\tassert(sentence[word_index].get('lemma') == word_lemma)\n",
    "\n",
    "\t\t\t# the clean sentence in list\n",
    "\t\t\tclean_sentence = [word_dict.get('lemma') for word_dict in sentence]\n",
    "\t\t\t# print(clean_sentence)\n",
    "\t\t\t# print(len(clean_sentence))\n",
    "\t\t\ttest_sentences.append(clean_sentence)\n",
    "\t\t\ttest_word_index.append(word_index)\n",
    "\t\t\ttest_word_sense.append(word_sense)\n",
    "\n",
    "\t\telse:\n",
    "\t\t\tsentence = dev_data[sentence_number]\n",
    "\t\t\t# print(sentence)\n",
    "\t\t\tassert(sentence[word_index].get('lemma') == word_lemma)\n",
    "\n",
    "\t\t\t# the clean sentence in list\n",
    "\t\t\tclean_sentence = [word_dict.get('lemma') for word_dict in sentence]\n",
    "\t\t\t# print(clean_sentence)\n",
    "\t\t\t# print(len(clean_sentence))\n",
    "\t\t\tdev_sentences.append(clean_sentence)\n",
    "\t\t\tdev_word_index.append(word_index)\n",
    "\t\t\tdev_word_sense.append(word_sense)\n",
    "\n",
    "\tprint('Processed {} sentences for test purpose.'.format(sen_num))\n",
    "\tprint('\\n******************* Data Example ***********************')\n",
    "\tprint('Sentence: {}'.format(train_sentences[0]))\n",
    "\tprint('Target Word Index: {}'.format(train_word_index[0]))\n",
    "\tprint('Target Word Sense (index in WordNet 3.1): {}'.format(train_word_sense[0]))\n",
    "\tprint('********************************************************')\n",
    "\n",
    "\treturn train_sentences, train_word_sense, train_word_index, test_sentences, test_word_sense, test_word_index, dev_sentences, dev_word_sense, dev_word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the WSD dataset first\n",
    "# and retrieve all sentences from the EUD\n",
    "def parse_data():\n",
    "\t\n",
    "\t# parse the WSD dataset\n",
    "\twsd_data = []\n",
    "\n",
    "\t# read in tsv by White et. al., 2016\n",
    "\twith open('data/wsd/wsd_eng_ud1.2_10262016.tsv', mode = 'r') as wsd_file:\n",
    "\n",
    "\t\ttsv_reader = csv.DictReader(wsd_file, delimiter = '\\t')\n",
    "\n",
    "\t\t# store the data\n",
    "\t\tfor row in tsv_reader:\n",
    "\n",
    "\t\t\t# each data vector\n",
    "\t\t\twsd_data.append(row)\n",
    "\n",
    "\t\t# make sure all data are parsed\n",
    "\t\tprint('Parsed {} word sense data from White et. al., 2016.'.format(len(wsd_data)))\n",
    "\n",
    "\t# parse the EUD-EWT conllu files and retrieve the sentences\n",
    "\ttrain_file = open(\"data/UD_English-EWT/en_ewt-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\ttrain_data = list(parse_incr(train_file))\n",
    "\tprint('Parsed {} training data from UD_English-EWT/en_ewt-ud-train.conllu.'.format(len(train_data)))\n",
    "\t# 'spring' as the first example in White et. al., 2016\n",
    "\t# print(train_data[1363])\n",
    "\n",
    "\ttest_file = open(\"data/UD_English-EWT/en_ewt-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\ttest_data = list(parse_incr(test_file))\n",
    "\tprint('Parsed {} testing data from UD_English-EWT/en_ewt-ud-test.conllu.'.format(len(test_data)))\n",
    "\n",
    "\tdev_file = open(\"data/UD_English-EWT/en_ewt-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "\tdev_data = list(parse_incr(dev_file))\n",
    "\tprint('Parsed {} dev data from UD_English-EWT/en_ewt-ud-dev.conllu.'.format(len(dev_data)))\n",
    "\n",
    "\treturn wsd_data, train_data, test_data, dev_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 439312 word sense data from White et. al., 2016.\n",
      "Parsed 12543 training data from UD_English-EWT/en_ewt-ud-train.conllu.\n",
      "Parsed 2077 testing data from UD_English-EWT/en_ewt-ud-test.conllu.\n",
      "Parsed 2002 dev data from UD_English-EWT/en_ewt-ud-dev.conllu.\n",
      "Processed 20 sentences for test purpose.\n",
      "\n",
      "******************* Data Example ***********************\n",
      "Sentence: ['on', 'August', '9', ',', '2004', ',', 'it', 'be', 'announce', 'that', 'in', 'the', 'spring', 'of', '2001', ',', 'a', 'man', 'name', 'El', '-', 'Shukrijumah', ',', 'also', 'know', 'as', 'Jafar', 'the', 'Pilot', ',', 'who', 'be', 'part', 'of', 'a', '\"', 'second', 'wave', ',', '\"', 'have', 'be', 'case', 'New', 'York', 'City', 'helicopter', '.']\n",
      "Target Word Index: 12\n",
      "Target Word Sense (index in WordNet 3.1): spring.n.01\n",
      "********************************************************\n"
     ]
    }
   ],
   "source": [
    "# parse the data\n",
    "wsd_data, train_data, test_data, dev_data = parse_data()\n",
    "\n",
    "# return the raw sentences from the EUD for train, test, and dev\n",
    "# test the first 20 sentences\n",
    "train_sentences, train_word_sense, train_word_index, test_sentences, test_word_sense, test_word_index, dev_sentences, dev_word_sense, dev_word_index = get_raw_sentences(wsd_data, train_data, test_data, dev_data, 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All parameters in the model:\n",
      "layers.WSD.0.weight torch.Size([300, 512])\n",
      "layers.WSD.0.bias torch.Size([300])\n",
      "layers.WSD.2.weight torch.Size([10, 300])\n",
      "layers.WSD.2.bias torch.Size([10])\n",
      "dimension_reduction_MLP.weight torch.Size([256, 3072])\n",
      "dimension_reduction_MLP.bias torch.Size([256])\n",
      "wsd_lstm.weight_ih_l0 torch.Size([1024, 256])\n",
      "wsd_lstm.weight_hh_l0 torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l0 torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l0 torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l0_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.weight_hh_l0_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l0_reverse torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l0_reverse torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l1 torch.Size([1024, 512])\n",
      "wsd_lstm.weight_hh_l1 torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l1 torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l1 torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l1_reverse torch.Size([1024, 512])\n",
      "wsd_lstm.weight_hh_l1_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l1_reverse torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l1_reverse torch.Size([1024])\n",
      "Model(\n",
      "  (layers): ModuleDict(\n",
      "    (WSD): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=300, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=300, out_features=10, bias=True)\n",
      "      (3): Softmax()\n",
      "    )\n",
      "  )\n",
      "  (mlp_dropout): Dropout(p=0)\n",
      "  (dimension_reduction_MLP): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (wsd_lstm): LSTM(256, 256, num_layers=2, bidirectional=True)\n",
      ")\n",
      "\n",
      "******************* fine-tuning MLP structure ***********************\n",
      "Current Task: WSD\n",
      "Linear(in_features=512, out_features=300, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=300, out_features=10, bias=True)\n",
      "Softmax()\n",
      "**********************************************************************\n",
      "\n",
      "Original ELMo embeddings size: torch.Size([20, 3, 48, 1024]), mask: torch.Size([20, 48])\n",
      "Embedding size after dimension reduction: torch.Size([48, 20, 256]), mask: torch.Size([48, 20, 256])\n",
      "\n",
      "Embedding size after bi-LSTM: torch.Size([48, 20, 512])\n",
      "\n",
      "Output of the fine-tuning MLP: \n",
      "Total 20 words. \n",
      "Each word has a 10-d vector output as probabilities distributing over its senses: torch.Size([10])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0977, 0.1017, 0.0975, 0.0967, 0.0965, 0.1046, 0.1032, 0.1002, 0.0981,\n",
       "         0.1039], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0971, 0.1014, 0.0967, 0.0971, 0.0973, 0.1055, 0.1040, 0.0996, 0.0979,\n",
       "         0.1034], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0971, 0.1014, 0.0967, 0.0971, 0.0973, 0.1055, 0.1040, 0.0996, 0.0979,\n",
       "         0.1034], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0967, 0.1012, 0.0976, 0.0966, 0.0970, 0.1052, 0.1028, 0.1004, 0.0982,\n",
       "         0.1045], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0967, 0.1012, 0.0976, 0.0966, 0.0970, 0.1052, 0.1028, 0.1004, 0.0982,\n",
       "         0.1045], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0967, 0.1012, 0.0976, 0.0966, 0.0970, 0.1052, 0.1028, 0.1004, 0.0982,\n",
       "         0.1045], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0967, 0.1012, 0.0976, 0.0966, 0.0970, 0.1052, 0.1028, 0.1004, 0.0982,\n",
       "         0.1045], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0971, 0.1004, 0.0981, 0.0968, 0.0963, 0.1048, 0.1036, 0.1003, 0.0984,\n",
       "         0.1043], grad_fn=<SoftmaxBackward>),\n",
       " tensor([0.0971, 0.1004, 0.0981, 0.0968, 0.0963, 0.1048, 0.1036, 0.1003, 0.0984,\n",
       "         0.1043], grad_fn=<SoftmaxBackward>)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ELMo setup\n",
    "# ELMo is tuned to lower dimension (256) by MLP in Model\n",
    "elmo = ElmoEmbedder()\n",
    "model = Model(elmo_class = elmo)\n",
    "\n",
    "# print the model \n",
    "print_whole_model(model)\n",
    "\n",
    "# MLP illustration\n",
    "print_fine_tuning_MLP(model, 'WSD')\n",
    "\n",
    "# forward propagation\n",
    "# ELMo (1024) -> dimension reduction (256) -> bi-LSTM (512) -> fine-tuning MLP (10)\n",
    "model.forward(train_sentences, train_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
