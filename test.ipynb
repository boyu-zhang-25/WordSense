{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import string\n",
    "import itertools\n",
    "from io import open\n",
    "from conllu import parse_incr\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the structure of the fine-tuning MLP for WSD\n",
    "def print_fine_tuning_MLP(model, param):\n",
    "\n",
    "\tprint('\\n******************* fine-tuning MLP structure ***********************')\n",
    "\n",
    "\tprint('Current Task: {}'.format(param))\n",
    "\tmodule_dict = model.layers[param]\n",
    "\n",
    "\tfor module in module_dict:\n",
    "\t\tprint(module)\n",
    "\n",
    "\tprint('**********************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the whole model \n",
    "def print_whole_model(model):\n",
    "\n",
    "\tprint('\\nAll parameters in the model:')\n",
    "\tfor name, param in model.named_parameters():\n",
    "\t\tif param.requires_grad:\n",
    "\t\t\tprint(name, param.size())\n",
    "\n",
    "\tprint(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return: \n",
    "all sentences\n",
    "target word index\n",
    "target word lemma\n",
    "target word sense\n",
    "all senses for each word \n",
    "from the EUD for train, test, and dev dataset\n",
    "index provided by WSD dataset by White et. al.\n",
    "'''\n",
    "# get all the senses and definitions for each word from WSD dataset\n",
    "# order of senses and definitions are in order\n",
    "def get_all_senses_and_definitions(wsd_data):\n",
    "\n",
    "    # all senses for each \n",
    "    all_senses = {}\n",
    "    all_definitions = {}\n",
    "    \n",
    "    # for test purpose: only load specific amount of data\n",
    "    for i in range(45):\n",
    "\n",
    "        # get the original sentence from EUD\n",
    "        sentence_id = wsd_data[i].get('Sentence.ID')\n",
    "        \n",
    "        # remove punctuations from definitions?\n",
    "        definition = wsd_data[i].get('Sense.Definition').translate(str.maketrans('', '', string.punctuation))\n",
    "        \n",
    "        # the index in EUD is 1-based!!!\n",
    "        sentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "        word_index = int(wsd_data[i].get('Arg.Token')) - 1\n",
    "        word_lemma = wsd_data[i].get('Arg.Lemma')\n",
    "        word_sense = wsd_data[i].get('Synset')\n",
    "        response = wsd_data[i].get('Sense.Response')\n",
    "\n",
    "        # if the word already exits: add the new sense to the list\n",
    "        # else: creata a new list for the word\n",
    "        if word_lemma in all_senses.keys():\n",
    "            if word_sense not in all_senses[word_lemma]:\n",
    "                all_senses[word_lemma].append(word_sense)\n",
    "        else:\n",
    "            all_senses[word_lemma] = []\n",
    "            all_senses[word_lemma].append(word_sense)            \n",
    "            \n",
    "        if word_lemma in all_definitions.keys():\n",
    "            if definition not in all_definitions[word_lemma]:\n",
    "                # split to list before store\n",
    "                all_definitions[word_lemma].append(definition.split(' '))\n",
    "        else:\n",
    "            all_definitions[word_lemma] = []\n",
    "            all_definitions[word_lemma].append(definition.split(' '))\n",
    "        \n",
    "    return all_senses, all_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the WSD dataset first\n",
    "# and retrieve all sentences from the EUD\n",
    "\n",
    "'''\n",
    "Copyright@\n",
    "White, A. S., D. Reisinger, K. Sakaguchi, T. Vieira, S. Zhang, R. Rudinger, K. Rawlins, & B. Van Durme. 2016. \n",
    "[Universal decompositional semantics on universal dependencies]\n",
    "(http://aswhite.net/media/papers/white_universal_2016.pdf). \n",
    "To appear in *Proceedings of the Conference on Empirical Methods in Natural Language Processing 2016*.\n",
    "'''\n",
    "\n",
    "# parse the WSD dataset and construct X_Y tensors\n",
    "def parse_data():\n",
    "\n",
    "    # parse the EUD-EWT conllu files and retrieve the sentences\n",
    "    # remove all punctuation?\n",
    "    train_file = open(\"data/UD_English-EWT/en_ewt-ud-train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    train_data = list(parse_incr(train_file))\n",
    "    # train_data = [[''.join(c for c in word.get('lemma') if c not in string.punctuation) for word in token_list] for token_list in train_data]\n",
    "    # train_data = [[word for word in s if word] for s in train_data]\n",
    "    print('Parsed {} training data from UD_English-EWT/en_ewt-ud-train.conllu.'.format(len(train_data)))\n",
    "\n",
    "    test_file = open(\"data/UD_English-EWT/en_ewt-ud-test.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    test_data = list(parse_incr(test_file))\n",
    "    # test_data = [[''.join(c for c in word.get('lemma') if c not in string.punctuation) for word in token_list] for token_list in test_data]\n",
    "    # test_data = [[word for word in s if word] for s in test_data]\n",
    "    print('Parsed {} testing data from UD_English-EWT/en_ewt-ud-test.conllu.'.format(len(test_data)))\n",
    "\n",
    "    dev_file = open(\"data/UD_English-EWT/en_ewt-ud-dev.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    dev_data = list(parse_incr(dev_file))\n",
    "    # dev_data = [[''.join(c for c in word.get('lemma') if c not in string.punctuation) for word in token_list] for token_list in dev_data]\n",
    "    # dev_data = [[word for word in s if word] for s in dev_data]\n",
    "    print('Parsed {} dev data from UD_English-EWT/en_ewt-ud-dev.conllu.'.format(len(dev_data)))\n",
    "\n",
    "    # parse the WSD dataset\n",
    "    wsd_data = []\n",
    "\n",
    "    # read in tsv by White et. al., 2016\n",
    "    with open('data/wsd/wsd_eng_ud1.2_10262016.tsv', mode = 'r') as wsd_file:\n",
    "\n",
    "        tsv_reader = csv.DictReader(wsd_file, delimiter = '\\t')      \n",
    "\n",
    "        # store the data: ordered dict row\n",
    "        for row in tsv_reader:                                \n",
    "\n",
    "            # each data vector\n",
    "            wsd_data.append(row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print('Parsed {} word sense data from White et. al., 2016.'.format(len(wsd_data)))\n",
    "\n",
    "    return wsd_data, train_data, test_data, dev_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 12543 training data from UD_English-EWT/en_ewt-ud-train.conllu.\n",
      "Parsed 2077 testing data from UD_English-EWT/en_ewt-ud-test.conllu.\n",
      "Parsed 2002 dev data from UD_English-EWT/en_ewt-ud-dev.conllu.\n",
      "Parsed 439312 word sense data from White et. al., 2016.\n"
     ]
    }
   ],
   "source": [
    "# parse the data\n",
    "wsd_data, train_data, test_data, dev_data = parse_data()\n",
    "\n",
    "# return the raw sentences from the EUD for train, test, and dev\n",
    "# test the first 20 sentences\n",
    "all_senses, all_definitions = get_all_senses_and_definitions(wsd_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Construct the X and Y for train, dev, and test from White et. al., 2016\n",
    "For each anonator and each word, on pair of data and label will be created\n",
    "Warning: code here is hard to read LMAO\n",
    "'''\n",
    "def construct_X_Y(all_senses, all_definitions, train_data, dev_data, test_data):\n",
    "    \n",
    "    wsd_data = []\n",
    "    \n",
    "    with open('data/wsd/wsd_eng_ud1.2_10262016.tsv', mode = 'r') as wsd_file:\n",
    "        \n",
    "        tsv_reader = csv.DictReader(wsd_file, delimiter = '\\t')\n",
    "        \n",
    "        # same annotator and same sentence number will generate on pair of X_Y\n",
    "        # manually set the first sentence from White et. al., 2016\n",
    "        current_annotator = '0'\n",
    "        current_sentence_num = '1364'\n",
    "        current_Y = [0 for _ in range(len(all_senses['spring']))]           \n",
    "        sentence = train_data[1363]\n",
    "        \n",
    "        # word from the EUD is a ordered dict for word properties\n",
    "        # use key 'lemma' to get the literal representations\n",
    "        current_X = [word.get('lemma') for word in sentence]\n",
    "        current_idx = 12\n",
    "        \n",
    "        # lists X and Y\n",
    "        train_X, test_X, dev_X = ([] for i in range(3))\n",
    "        train_Y, test_Y, dev_Y = ([] for i in range(3))\n",
    "        train_word_idx, test_word_idx, dev_word_idx = ([] for i in range(3))\n",
    "\n",
    "        for idx, row in enumerate(tsv_reader):\n",
    "                        \n",
    "            # training set; only test first 30 training sentences for now\n",
    "            if idx < 45 and row['Split'] == 'train':\n",
    "                \n",
    "                # if still is the same annotatior, word index, target word\n",
    "                # modify Y with the sense reponse\n",
    "                if current_annotator == row['Annotator.ID'] and current_idx == int(row['Arg.Token']) - 1 and current_sentence_num == row['Sentence.ID'].split(' ')[-1]:\n",
    "                    \n",
    "                    sense_idx = all_senses[row['Arg.Lemma']].index(row['Synset'])\n",
    "                    if row['Sense.Response'] == 'True':\n",
    "                        current_Y[sense_idx] = 1\n",
    "                    else:\n",
    "                        current_Y[sense_idx] = 0\n",
    "                \n",
    "                # if switch annotator or target word\n",
    "                # append the Y and X from the last annotator and word\n",
    "                # start a new Y and X for the current annotator and target\n",
    "                else:\n",
    "                    # print('h2: {}'.format(idx))\n",
    "                    train_X.append(current_X)\n",
    "                    train_Y.append(current_Y)\n",
    "                    train_word_idx.append(current_idx)\n",
    "                    \n",
    "                    current_annotator = row['Annotator.ID']\n",
    "                    current_sentence_num = row['Sentence.ID'].split(' ')[-1]\n",
    "                    \n",
    "                    current_idx = int(row['Arg.Token']) - 1\n",
    "                    current_Y = [0 for _ in range(len(all_senses[row['Arg.Lemma']]))]\n",
    "                    sense_idx = all_senses[row['Arg.Lemma']].index(row['Synset'])\n",
    "                    if row['Sense.Response'] == 'True':\n",
    "                        current_Y[sense_idx] = 1\n",
    "                    else:\n",
    "                        current_Y[sense_idx] = 0\n",
    "                    \n",
    "                    sentence_id = row['Sentence.ID']\n",
    "                    sentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "                    sentence = train_data[sentence_number]\n",
    "                    current_X = [word.get('lemma') for word in sentence]\n",
    "                    \n",
    "            # testing set\n",
    "            elif idx < 45 and row['Split'] == 'test':\n",
    "                \n",
    "                if current_annotator == row['Annotator.ID'] and current_idx == int(row['Arg.Token']) - 1 and current_sentence_num == row['Sentence.ID'].split(' ')[-1]:\n",
    "                    \n",
    "                    sense_idx = all_senses[row['Arg.Lemma']].index(row['Synset'])\n",
    "                    if row['Sense.Response'] == 'True':\n",
    "                        current_Y[sense_idx] = 1\n",
    "                    else:\n",
    "                        current_Y[sense_idx] = 0\n",
    "                else:\n",
    "                    test_X.append(current_X)\n",
    "                    test_Y.append(current_Y)\n",
    "                    test_word_idx.append(current_idx)\n",
    "\n",
    "                    current_annotator = row['Annotator.ID']\n",
    "                    current_sentence_num = row['Sentence.ID'].split(' ')[-1]\n",
    "\n",
    "                    current_idx = int(row['Arg.Token']) - 1\n",
    "                    current_Y = [0 for _ in range(len(all_senses[row['Arg.Lemma']]))]\n",
    "                    sense_idx = all_senses[row['Arg.Lemma']].index(row['Synset'])\n",
    "                    if row['Sense.Response'] == 'True':\n",
    "                        current_Y[sense_idx] = 1\n",
    "                    else:\n",
    "                        current_Y[sense_idx] = 0\n",
    "                    \n",
    "                    sentence_id = row['Sentence.ID']\n",
    "                    sentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "                    sentence = test_data[sentence_number]\n",
    "                    current_X = [word.get('lemma') for word in sentence]\n",
    "                    \n",
    "            # dev set       \n",
    "            elif idx < 45:\n",
    "                if current_annotator == row['Annotator.ID'] and current_idx == int(row['Arg.Token']) - 1 and current_sentence_num == row['Sentence.ID'].split(' ')[-1]:\n",
    "                    \n",
    "                    sense_idx = all_senses[row['Arg.Lemma']].index(row['Synset'])\n",
    "                    if row['Sense.Response'] == 'True':\n",
    "                        current_Y[sense_idx] = 1\n",
    "                    else:\n",
    "                        current_Y[sense_idx] = 0\n",
    "                else:\n",
    "                    dev_X.append(current_X)\n",
    "                    dev_Y.append(current_Y)\n",
    "                    dev_word_idx.append(current_idx)\n",
    "\n",
    "                    current_annotator = row['Annotator.ID']\n",
    "                    current_sentence_num = row['Sentence.ID'].split(' ')[-1]\n",
    "\n",
    "                    current_idx = int(row['Arg.Token']) - 1\n",
    "                    current_Y = [0 for _ in range(len(all_senses[row['Arg.Lemma']]))]\n",
    "                    sense_idx = all_senses[row['Arg.Lemma']].index(row['Synset'])\n",
    "                    if row['Sense.Response'] == 'True':\n",
    "                        current_Y[sense_idx] = 1\n",
    "                    else:\n",
    "                        current_Y[sense_idx] = 0\n",
    "                    \n",
    "                    sentence_id = row['Sentence.ID']\n",
    "                    sentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "                    sentence = dev_data[sentence_number]\n",
    "                    current_X = [word.get('lemma') for word in sentence]\n",
    "        \n",
    "        print('\\n******************* Data Example ***********************')\n",
    "        print('Sentence: {}'.format(train_X[0]))\n",
    "        print('Annotator Response, i.e., true label: {}'.format(train_Y[0]))\n",
    "        print('Target Word Index: {}'.format(train_word_idx[0]))\n",
    "        print('All senses for the target word: {}'.format(all_senses[train_X[0][train_word_idx[0]]]))\n",
    "        print('All definitions (in order of its senses from WordNet): {}'.format(all_definitions[train_X[0][train_word_idx[0]]]))\n",
    "        print('********************************************************')\n",
    "        \n",
    "        return train_X, test_X, dev_X, train_Y, test_Y, dev_Y, train_word_idx, test_word_idx, dev_word_idx\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "******************* Data Example ***********************\n",
      "Sentence: ['on', 'August', '9', ',', '2004', ',', 'it', 'be', 'announce', 'that', 'in', 'the', 'spring', 'of', '2001', ',', 'a', 'man', 'name', 'El', '-', 'Shukrijumah', ',', 'also', 'know', 'as', 'Jafar', 'the', 'Pilot', ',', 'who', 'be', 'part', 'of', 'a', '\"', 'second', 'wave', ',', '\"', 'have', 'be', 'case', 'New', 'York', 'City', 'helicopter', '.']\n",
      "Annotator Response, i.e., true label: [1, 0, 0, 0, 0, 0]\n",
      "Target Word Index: 12\n",
      "All senses for the target word: ['spring.n.01', 'spring.n.02', 'spring.n.03', 'spring.n.04', 'give.n.01', 'leap.n.01']\n",
      "All definitions (in order of its senses from WordNet): [['the', 'season', 'of', 'growth'], ['a', 'metal', 'elastic', 'device', 'that', 'returns', 'to', 'its', 'shape', 'or', 'position', 'when', 'pushed', 'or', 'pulled', 'or', 'pressed'], ['a', 'natural', 'flow', 'of', 'ground', 'water'], ['a', 'point', 'at', 'which', 'water', 'issues', 'forth'], ['the', 'elasticity', 'of', 'something', 'that', 'can', 'be', 'stretched', 'and', 'returns', 'to', 'its', 'original', 'length'], ['a', 'light', 'selfpropelled', 'movement', 'upwards', 'or', 'forwards'], ['the', 'season', 'of', 'growth'], ['a', 'metal', 'elastic', 'device', 'that', 'returns', 'to', 'its', 'shape', 'or', 'position', 'when', 'pushed', 'or', 'pulled', 'or', 'pressed'], ['a', 'natural', 'flow', 'of', 'ground', 'water'], ['a', 'point', 'at', 'which', 'water', 'issues', 'forth'], ['the', 'elasticity', 'of', 'something', 'that', 'can', 'be', 'stretched', 'and', 'returns', 'to', 'its', 'original', 'length'], ['a', 'light', 'selfpropelled', 'movement', 'upwards', 'or', 'forwards']]\n",
      "********************************************************\n"
     ]
    }
   ],
   "source": [
    "# get the X and Y for models\n",
    "train_X, test_X, dev_X, train_Y, test_Y, dev_Y, train_word_idx, test_word_idx, dev_word_idx = construct_X_Y(all_senses, all_definitions, train_data, dev_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All parameters in the model:\n",
      "layers.word_sense.0.weight torch.Size([512, 512])\n",
      "layers.word_sense.0.bias torch.Size([512])\n",
      "layers.word_sense.2.weight torch.Size([300, 512])\n",
      "layers.word_sense.2.bias torch.Size([300])\n",
      "dimension_reduction_MLP.weight torch.Size([256, 3072])\n",
      "dimension_reduction_MLP.bias torch.Size([256])\n",
      "wsd_lstm.weight_ih_l0 torch.Size([1024, 256])\n",
      "wsd_lstm.weight_hh_l0 torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l0 torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l0 torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l0_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.weight_hh_l0_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l0_reverse torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l0_reverse torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l1 torch.Size([1024, 512])\n",
      "wsd_lstm.weight_hh_l1 torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l1 torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l1 torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l1_reverse torch.Size([1024, 512])\n",
      "wsd_lstm.weight_hh_l1_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l1_reverse torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l1_reverse torch.Size([1024])\n",
      "Model(\n",
      "  (layers): ModuleDict(\n",
      "    (word_sense): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=512, out_features=300, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (mlp_dropout): Dropout(p=0)\n",
      "  (dimension_reduction_MLP): Linear(in_features=3072, out_features=256, bias=True)\n",
      "  (wsd_lstm): LSTM(256, 256, num_layers=2, bidirectional=True)\n",
      ")\n",
      "\n",
      "******************* fine-tuning MLP structure ***********************\n",
      "Current Task: word_sense\n",
      "Linear(in_features=512, out_features=512, bias=True)\n",
      "ReLU()\n",
      "Linear(in_features=512, out_features=300, bias=True)\n",
      "**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# ELMo setup\n",
    "# ELMo is tuned to lower dimension (256) by MLP in Model\n",
    "elmo = ElmoEmbedder()\n",
    "model = Model(elmo_class = elmo, all_senses = all_senses)\n",
    "\n",
    "# print the model \n",
    "print_whole_model(model)\n",
    "\n",
    "# MLP illustration\n",
    "print_fine_tuning_MLP(model, 'word_sense')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original ELMo embeddings size: torch.Size([8, 3, 48, 1024]), mask: torch.Size([8, 48])\n",
      "Embedding size after dimension reduction: torch.Size([48, 8, 256]), mask: torch.Size([48, 8, 256])\n",
      "\n",
      "Embedding size after bi-LSTM: torch.Size([48, 8, 512])\n",
      "\n",
      "Word lemma: spring\n",
      "Word sense embedding: tensor([ 2.3963e-02, -5.4756e-03,  1.8193e-02, -3.6550e-03,  8.4025e-03,\n",
      "         1.7008e-02,  1.1617e-02,  8.9106e-03,  1.8704e-02,  2.2489e-02,\n",
      "         3.1534e-02,  1.4566e-02,  3.1566e-02, -3.8270e-02, -3.2203e-02,\n",
      "         3.4717e-02,  2.1209e-03,  4.2550e-02, -3.1096e-02, -1.4682e-02,\n",
      "         3.6027e-02, -1.7327e-03,  2.7812e-02, -4.0330e-02,  2.7222e-02,\n",
      "        -5.3606e-02, -2.6602e-02,  2.3869e-02,  2.4150e-02,  7.9192e-03,\n",
      "         1.3423e-02,  4.5001e-03,  4.7445e-02, -2.3569e-02,  7.2848e-03,\n",
      "         3.4808e-02, -4.0729e-02, -2.4535e-03,  1.8732e-02, -2.9831e-02,\n",
      "        -2.2471e-02,  4.8625e-02,  1.2042e-02, -2.2454e-02,  6.3223e-02,\n",
      "        -3.2269e-02, -1.5492e-02,  5.8965e-02,  2.1468e-02, -1.4518e-02,\n",
      "         2.0224e-02, -2.8897e-02,  2.8467e-02,  2.4815e-02,  1.3211e-02,\n",
      "        -2.8419e-03, -6.5468e-05, -3.6976e-02, -8.1217e-03, -1.2495e-02,\n",
      "         5.0982e-02,  1.8509e-02, -1.1384e-02, -5.0499e-02, -9.4663e-03,\n",
      "         4.6521e-02,  6.6477e-03, -1.2798e-02,  1.8310e-02,  4.2734e-02,\n",
      "        -2.4447e-02,  4.6047e-02, -1.6239e-02, -2.3353e-02, -1.6225e-02,\n",
      "         9.2010e-04,  1.8102e-03,  1.3656e-02,  3.2183e-02, -2.5249e-03,\n",
      "         2.7371e-03, -2.1771e-02, -5.3877e-03,  1.0651e-02,  3.3443e-02,\n",
      "        -7.6189e-03, -8.6857e-03, -2.2265e-02,  6.9495e-02, -1.6927e-02,\n",
      "        -3.0072e-02, -1.5475e-02, -1.3945e-02, -7.8056e-04, -1.9943e-02,\n",
      "        -3.8361e-02,  1.2135e-02,  5.1861e-02,  2.9065e-02,  4.7628e-02,\n",
      "         3.8738e-02, -1.0900e-02, -4.4861e-02, -1.9182e-02,  1.5941e-02,\n",
      "        -4.1402e-02, -3.1153e-02, -4.0190e-03,  6.5116e-03,  4.0180e-02,\n",
      "         1.8623e-02,  3.7391e-02,  4.5484e-02, -1.3260e-03,  5.9720e-03,\n",
      "         3.3977e-02, -5.6986e-02,  5.9860e-04,  1.5925e-02,  5.9622e-02,\n",
      "         3.4031e-02, -1.5184e-02,  3.0762e-02, -3.1649e-02,  2.4450e-02,\n",
      "         3.2058e-02,  2.1456e-02,  2.2885e-02, -1.6702e-02, -4.0347e-03,\n",
      "        -1.0071e-02,  3.8978e-02, -3.6975e-02,  3.7478e-02, -3.5235e-03,\n",
      "         1.9407e-02, -1.5610e-02,  3.0685e-02, -1.2725e-02, -1.0434e-02,\n",
      "        -1.6224e-02,  3.3827e-02,  2.2878e-02, -2.5256e-02, -1.7751e-02,\n",
      "         3.6501e-02, -4.3970e-02, -3.6927e-02, -5.4530e-05,  3.4420e-02,\n",
      "         1.2083e-02, -1.2805e-02, -1.0049e-02, -7.7492e-03,  2.5927e-02,\n",
      "         4.8565e-02,  5.0549e-02,  3.1174e-02,  1.1008e-02, -2.7231e-03,\n",
      "         3.5385e-02,  1.2273e-02, -4.5399e-03,  1.3375e-02, -3.8889e-02,\n",
      "         2.8715e-02,  3.2008e-02, -2.8378e-02, -1.4013e-02, -2.7874e-02,\n",
      "         1.0640e-02, -1.2861e-02, -1.3670e-02,  1.5437e-02,  8.0147e-02,\n",
      "        -1.1145e-03,  1.1320e-03,  1.5525e-02,  1.1332e-03,  1.7445e-03,\n",
      "        -6.9854e-03, -1.3554e-02, -1.4050e-02,  3.0813e-02, -2.7670e-03,\n",
      "        -2.7195e-02,  6.0915e-03, -4.1181e-02, -3.7161e-02,  5.1398e-02,\n",
      "         1.3049e-03, -3.1001e-02,  2.3667e-02,  2.7326e-02,  2.5597e-03,\n",
      "         1.8461e-02, -2.2101e-02, -3.8233e-02, -4.7392e-02,  2.4913e-02,\n",
      "        -5.1392e-02, -8.9034e-03,  3.8486e-02, -2.9090e-02,  1.9568e-02,\n",
      "        -1.6880e-02,  7.1610e-03, -3.8203e-02, -2.2066e-02,  2.7873e-02,\n",
      "        -2.1677e-02,  1.3395e-02, -4.8528e-02, -2.2845e-02, -3.6877e-02,\n",
      "        -2.9103e-02, -3.6449e-02, -1.6303e-02,  1.2591e-02, -5.1632e-02,\n",
      "        -1.6732e-02,  2.1010e-02, -3.9680e-02, -2.4727e-02, -2.3933e-03,\n",
      "        -2.0055e-02, -2.9707e-03,  3.2637e-02, -8.4975e-03, -4.1422e-02,\n",
      "        -2.4212e-02,  3.7903e-03, -5.8502e-03,  2.2944e-02,  1.2109e-02,\n",
      "         1.3250e-02,  1.3793e-02,  1.6754e-02, -2.7890e-02, -2.3222e-02,\n",
      "         7.5625e-03,  2.5445e-03, -3.5296e-02, -5.3567e-02, -5.8893e-02,\n",
      "        -5.0652e-02, -5.3485e-04, -3.6999e-02,  3.3841e-02,  1.7800e-02,\n",
      "        -2.8725e-02, -1.1041e-02,  1.1598e-02,  1.8404e-02, -2.0835e-02,\n",
      "         6.3965e-03, -2.1842e-02,  1.0041e-02,  3.9215e-02, -3.6495e-02,\n",
      "        -1.2158e-02,  1.7213e-02,  3.6463e-03, -1.5837e-02, -4.7703e-02,\n",
      "         2.5192e-02,  2.7779e-02,  3.5067e-02,  3.4360e-03, -1.5912e-02,\n",
      "        -2.7894e-02,  2.7277e-03, -3.0225e-02,  1.2638e-02,  4.3606e-03,\n",
      "         7.9304e-03,  5.0290e-02,  5.8534e-02,  4.8090e-02, -5.9694e-03,\n",
      "         4.2635e-02, -4.2358e-02, -2.1508e-02, -1.5806e-02,  3.0187e-03,\n",
      "         2.7472e-02, -2.3434e-02, -6.0794e-03, -2.9209e-02,  2.1277e-02,\n",
      "         2.3926e-02,  2.5129e-02,  2.4667e-03,  3.9812e-02,  4.2975e-02,\n",
      "         2.5328e-03, -4.0951e-02,  1.3081e-02,  1.7665e-02, -3.6484e-02],\n",
      "       grad_fn=<AddBackward0>)\n",
      "All its senses: ['spring.n.01', 'spring.n.02', 'spring.n.03', 'spring.n.04', 'give.n.01', 'leap.n.01']\n",
      "\n",
      "Word lemma: spring\n",
      "Word sense embedding: tensor([ 2.3963e-02, -5.4756e-03,  1.8193e-02, -3.6550e-03,  8.4025e-03,\n",
      "         1.7008e-02,  1.1617e-02,  8.9106e-03,  1.8704e-02,  2.2489e-02,\n",
      "         3.1534e-02,  1.4566e-02,  3.1566e-02, -3.8270e-02, -3.2203e-02,\n",
      "         3.4717e-02,  2.1209e-03,  4.2550e-02, -3.1096e-02, -1.4682e-02,\n",
      "         3.6027e-02, -1.7327e-03,  2.7812e-02, -4.0330e-02,  2.7222e-02,\n",
      "        -5.3606e-02, -2.6602e-02,  2.3869e-02,  2.4150e-02,  7.9192e-03,\n",
      "         1.3423e-02,  4.5001e-03,  4.7445e-02, -2.3569e-02,  7.2848e-03,\n",
      "         3.4808e-02, -4.0729e-02, -2.4535e-03,  1.8732e-02, -2.9831e-02,\n",
      "        -2.2471e-02,  4.8625e-02,  1.2042e-02, -2.2454e-02,  6.3223e-02,\n",
      "        -3.2269e-02, -1.5492e-02,  5.8965e-02,  2.1468e-02, -1.4518e-02,\n",
      "         2.0224e-02, -2.8897e-02,  2.8467e-02,  2.4815e-02,  1.3211e-02,\n",
      "        -2.8419e-03, -6.5468e-05, -3.6976e-02, -8.1217e-03, -1.2495e-02,\n",
      "         5.0982e-02,  1.8509e-02, -1.1384e-02, -5.0499e-02, -9.4663e-03,\n",
      "         4.6521e-02,  6.6477e-03, -1.2798e-02,  1.8310e-02,  4.2734e-02,\n",
      "        -2.4447e-02,  4.6047e-02, -1.6239e-02, -2.3353e-02, -1.6225e-02,\n",
      "         9.2010e-04,  1.8102e-03,  1.3656e-02,  3.2183e-02, -2.5249e-03,\n",
      "         2.7371e-03, -2.1771e-02, -5.3877e-03,  1.0651e-02,  3.3443e-02,\n",
      "        -7.6189e-03, -8.6857e-03, -2.2265e-02,  6.9495e-02, -1.6927e-02,\n",
      "        -3.0072e-02, -1.5475e-02, -1.3945e-02, -7.8056e-04, -1.9943e-02,\n",
      "        -3.8361e-02,  1.2135e-02,  5.1861e-02,  2.9065e-02,  4.7628e-02,\n",
      "         3.8738e-02, -1.0900e-02, -4.4861e-02, -1.9182e-02,  1.5941e-02,\n",
      "        -4.1402e-02, -3.1153e-02, -4.0190e-03,  6.5116e-03,  4.0180e-02,\n",
      "         1.8623e-02,  3.7391e-02,  4.5484e-02, -1.3260e-03,  5.9720e-03,\n",
      "         3.3977e-02, -5.6986e-02,  5.9860e-04,  1.5925e-02,  5.9622e-02,\n",
      "         3.4031e-02, -1.5184e-02,  3.0762e-02, -3.1649e-02,  2.4450e-02,\n",
      "         3.2058e-02,  2.1456e-02,  2.2885e-02, -1.6702e-02, -4.0347e-03,\n",
      "        -1.0071e-02,  3.8978e-02, -3.6975e-02,  3.7478e-02, -3.5235e-03,\n",
      "         1.9407e-02, -1.5610e-02,  3.0685e-02, -1.2725e-02, -1.0434e-02,\n",
      "        -1.6224e-02,  3.3827e-02,  2.2878e-02, -2.5256e-02, -1.7751e-02,\n",
      "         3.6501e-02, -4.3970e-02, -3.6927e-02, -5.4530e-05,  3.4420e-02,\n",
      "         1.2083e-02, -1.2805e-02, -1.0049e-02, -7.7492e-03,  2.5927e-02,\n",
      "         4.8565e-02,  5.0549e-02,  3.1174e-02,  1.1008e-02, -2.7231e-03,\n",
      "         3.5385e-02,  1.2273e-02, -4.5399e-03,  1.3375e-02, -3.8889e-02,\n",
      "         2.8715e-02,  3.2008e-02, -2.8378e-02, -1.4013e-02, -2.7874e-02,\n",
      "         1.0640e-02, -1.2861e-02, -1.3670e-02,  1.5437e-02,  8.0147e-02,\n",
      "        -1.1145e-03,  1.1320e-03,  1.5525e-02,  1.1332e-03,  1.7445e-03,\n",
      "        -6.9854e-03, -1.3554e-02, -1.4050e-02,  3.0813e-02, -2.7670e-03,\n",
      "        -2.7195e-02,  6.0915e-03, -4.1181e-02, -3.7161e-02,  5.1398e-02,\n",
      "         1.3049e-03, -3.1001e-02,  2.3667e-02,  2.7326e-02,  2.5597e-03,\n",
      "         1.8461e-02, -2.2101e-02, -3.8233e-02, -4.7392e-02,  2.4913e-02,\n",
      "        -5.1392e-02, -8.9034e-03,  3.8486e-02, -2.9090e-02,  1.9568e-02,\n",
      "        -1.6880e-02,  7.1610e-03, -3.8203e-02, -2.2066e-02,  2.7873e-02,\n",
      "        -2.1677e-02,  1.3395e-02, -4.8528e-02, -2.2845e-02, -3.6877e-02,\n",
      "        -2.9103e-02, -3.6449e-02, -1.6303e-02,  1.2591e-02, -5.1632e-02,\n",
      "        -1.6732e-02,  2.1010e-02, -3.9680e-02, -2.4727e-02, -2.3933e-03,\n",
      "        -2.0055e-02, -2.9707e-03,  3.2637e-02, -8.4975e-03, -4.1422e-02,\n",
      "        -2.4212e-02,  3.7903e-03, -5.8502e-03,  2.2944e-02,  1.2109e-02,\n",
      "         1.3250e-02,  1.3793e-02,  1.6754e-02, -2.7890e-02, -2.3222e-02,\n",
      "         7.5625e-03,  2.5445e-03, -3.5296e-02, -5.3567e-02, -5.8893e-02,\n",
      "        -5.0652e-02, -5.3485e-04, -3.6999e-02,  3.3841e-02,  1.7800e-02,\n",
      "        -2.8725e-02, -1.1041e-02,  1.1598e-02,  1.8404e-02, -2.0835e-02,\n",
      "         6.3965e-03, -2.1842e-02,  1.0041e-02,  3.9215e-02, -3.6495e-02,\n",
      "        -1.2158e-02,  1.7213e-02,  3.6463e-03, -1.5837e-02, -4.7703e-02,\n",
      "         2.5192e-02,  2.7779e-02,  3.5067e-02,  3.4360e-03, -1.5912e-02,\n",
      "        -2.7894e-02,  2.7277e-03, -3.0225e-02,  1.2638e-02,  4.3606e-03,\n",
      "         7.9304e-03,  5.0290e-02,  5.8534e-02,  4.8090e-02, -5.9694e-03,\n",
      "         4.2635e-02, -4.2358e-02, -2.1508e-02, -1.5806e-02,  3.0187e-03,\n",
      "         2.7472e-02, -2.3434e-02, -6.0794e-03, -2.9209e-02,  2.1277e-02,\n",
      "         2.3926e-02,  2.5129e-02,  2.4667e-03,  3.9812e-02,  4.2975e-02,\n",
      "         2.5328e-03, -4.0951e-02,  1.3081e-02,  1.7665e-02, -3.6484e-02],\n",
      "       grad_fn=<AddBackward0>)\n",
      "All its senses: ['spring.n.01', 'spring.n.02', 'spring.n.03', 'spring.n.04', 'give.n.01', 'leap.n.01']\n",
      "\n",
      "Word lemma: ambition\n",
      "Word sense embedding: tensor([ 0.0254,  0.0034,  0.0222,  0.0023,  0.0078,  0.0201,  0.0147,  0.0056,\n",
      "         0.0241,  0.0154,  0.0268,  0.0061,  0.0373, -0.0348, -0.0166,  0.0353,\n",
      "         0.0118,  0.0448, -0.0233, -0.0275,  0.0372, -0.0088,  0.0275, -0.0470,\n",
      "         0.0200, -0.0558, -0.0416,  0.0308,  0.0192,  0.0045,  0.0132, -0.0062,\n",
      "         0.0352, -0.0197,  0.0069,  0.0425, -0.0432, -0.0021,  0.0316, -0.0304,\n",
      "        -0.0172,  0.0553,  0.0108, -0.0224,  0.0636, -0.0225, -0.0114,  0.0650,\n",
      "         0.0339, -0.0215,  0.0191, -0.0280,  0.0348,  0.0247,  0.0079, -0.0106,\n",
      "         0.0156, -0.0263, -0.0185, -0.0139,  0.0574,  0.0166,  0.0062, -0.0634,\n",
      "        -0.0116,  0.0411,  0.0078, -0.0136,  0.0179,  0.0415, -0.0149,  0.0456,\n",
      "        -0.0282, -0.0145, -0.0130, -0.0048,  0.0084,  0.0160,  0.0258, -0.0035,\n",
      "         0.0012, -0.0268, -0.0040,  0.0190,  0.0334, -0.0020, -0.0135, -0.0306,\n",
      "         0.0700, -0.0213, -0.0358, -0.0037, -0.0204, -0.0072, -0.0203, -0.0483,\n",
      "         0.0101,  0.0521,  0.0249,  0.0221,  0.0422, -0.0063, -0.0437, -0.0147,\n",
      "         0.0126, -0.0327, -0.0337, -0.0133,  0.0017,  0.0537,  0.0172,  0.0375,\n",
      "         0.0340, -0.0044,  0.0068,  0.0332, -0.0643, -0.0031,  0.0253,  0.0564,\n",
      "         0.0390, -0.0099,  0.0147, -0.0237,  0.0174,  0.0310,  0.0152,  0.0253,\n",
      "        -0.0039,  0.0021,  0.0008,  0.0443, -0.0472,  0.0339,  0.0061,  0.0056,\n",
      "        -0.0105,  0.0273, -0.0077, -0.0108, -0.0251,  0.0321,  0.0403, -0.0159,\n",
      "        -0.0130,  0.0406, -0.0385, -0.0502, -0.0009,  0.0272,  0.0154, -0.0115,\n",
      "        -0.0147,  0.0003,  0.0212,  0.0359,  0.0410,  0.0404,  0.0084,  0.0030,\n",
      "         0.0379,  0.0166, -0.0034,  0.0147, -0.0346,  0.0333,  0.0223, -0.0124,\n",
      "        -0.0141, -0.0416,  0.0076, -0.0151, -0.0150,  0.0231,  0.0669,  0.0132,\n",
      "        -0.0106,  0.0061, -0.0061, -0.0085, -0.0087, -0.0015, -0.0185,  0.0260,\n",
      "        -0.0065, -0.0277,  0.0041, -0.0445, -0.0407,  0.0560, -0.0013, -0.0289,\n",
      "         0.0216,  0.0435,  0.0066,  0.0212, -0.0226, -0.0354, -0.0372,  0.0308,\n",
      "        -0.0457, -0.0009,  0.0378, -0.0292,  0.0104, -0.0260,  0.0226, -0.0324,\n",
      "        -0.0316,  0.0297, -0.0248,  0.0241, -0.0371, -0.0303, -0.0348, -0.0336,\n",
      "        -0.0404, -0.0232, -0.0052, -0.0563, -0.0127,  0.0282, -0.0438, -0.0402,\n",
      "         0.0031, -0.0117, -0.0046,  0.0408, -0.0075, -0.0356, -0.0318,  0.0031,\n",
      "         0.0056,  0.0153,  0.0151,  0.0126,  0.0157,  0.0259, -0.0333, -0.0148,\n",
      "         0.0130,  0.0112, -0.0314, -0.0602, -0.0415, -0.0446, -0.0062, -0.0311,\n",
      "         0.0399,  0.0010, -0.0223, -0.0179,  0.0246,  0.0151, -0.0356,  0.0240,\n",
      "        -0.0226,  0.0113,  0.0421, -0.0382, -0.0031,  0.0229, -0.0046, -0.0161,\n",
      "        -0.0320,  0.0288,  0.0243,  0.0371,  0.0092, -0.0180, -0.0246, -0.0059,\n",
      "        -0.0313,  0.0096,  0.0043,  0.0114,  0.0441,  0.0534,  0.0451, -0.0143,\n",
      "         0.0436, -0.0298, -0.0115, -0.0180, -0.0024,  0.0251, -0.0229, -0.0231,\n",
      "        -0.0094,  0.0222,  0.0160,  0.0304,  0.0220,  0.0484,  0.0426,  0.0144,\n",
      "        -0.0349,  0.0102,  0.0222, -0.0426], grad_fn=<AddBackward0>)\n",
      "All its senses: ['ambition.n.01', 'ambition.n.02']\n",
      "\n",
      "Word lemma: management\n",
      "Word sense embedding: tensor([ 2.5534e-02,  3.4708e-03,  2.3494e-02, -7.6445e-04,  8.4899e-04,\n",
      "         2.2542e-02,  1.1691e-03,  8.4082e-04,  1.8722e-02,  1.6474e-02,\n",
      "         2.5857e-02,  1.0272e-02,  2.3435e-02, -3.5806e-02, -2.1796e-02,\n",
      "         3.3609e-02,  1.0061e-03,  4.7639e-02, -3.4437e-02, -2.4223e-02,\n",
      "         3.4464e-02, -3.7369e-03,  2.7371e-02, -5.4055e-02,  2.2487e-02,\n",
      "        -5.2070e-02, -3.6508e-02,  3.1559e-02,  1.8386e-02,  5.9208e-03,\n",
      "         1.2966e-02, -3.5360e-03,  4.5277e-02, -2.0263e-02,  5.2364e-03,\n",
      "         3.6781e-02, -4.7464e-02,  4.9241e-04,  2.0630e-02, -3.3964e-02,\n",
      "        -1.4893e-02,  5.4309e-02,  1.5966e-02, -1.9192e-02,  6.3491e-02,\n",
      "        -2.6885e-02, -1.2124e-02,  5.6685e-02,  1.6473e-02, -1.8161e-02,\n",
      "         2.4216e-02, -3.0243e-02,  2.6760e-02,  2.1545e-02,  1.1219e-02,\n",
      "        -1.1798e-02,  7.9294e-03, -2.4335e-02, -2.0969e-02, -5.5773e-03,\n",
      "         5.3417e-02,  2.1558e-02,  1.7595e-03, -5.8896e-02, -9.5363e-03,\n",
      "         4.4723e-02,  1.0212e-02, -1.5940e-02,  1.0538e-02,  4.2645e-02,\n",
      "        -1.5014e-02,  3.9265e-02, -3.0750e-02, -2.8170e-02, -2.0024e-02,\n",
      "        -2.1138e-03,  8.7330e-03,  1.5073e-02,  2.5225e-02, -1.1360e-02,\n",
      "        -1.9141e-03, -2.6477e-02, -2.2589e-03,  1.7649e-02,  4.0428e-02,\n",
      "         1.4371e-03, -8.2025e-03, -3.2969e-02,  7.4048e-02, -2.0435e-02,\n",
      "        -2.6900e-02, -4.8973e-03, -1.6035e-02, -1.1252e-03, -1.9425e-02,\n",
      "        -4.3546e-02,  9.7386e-03,  5.3575e-02,  2.2181e-02,  2.7515e-02,\n",
      "         4.0822e-02, -8.8872e-03, -4.3745e-02, -1.3805e-02,  1.5397e-02,\n",
      "        -3.1757e-02, -2.6561e-02, -8.2037e-03,  3.6318e-05,  4.7801e-02,\n",
      "         2.5834e-02,  3.8506e-02,  4.4954e-02,  6.2096e-03,  4.3167e-03,\n",
      "         3.5138e-02, -5.6427e-02,  1.1808e-03,  1.8212e-02,  5.4590e-02,\n",
      "         3.4851e-02, -1.5258e-02,  2.8995e-02, -2.2173e-02,  1.7633e-02,\n",
      "         2.6184e-02,  2.2648e-02,  2.5213e-02, -1.5948e-02, -1.8558e-03,\n",
      "        -1.0893e-02,  4.5293e-02, -3.5017e-02,  2.9594e-02,  2.0435e-03,\n",
      "         1.2927e-02, -1.7632e-02,  2.3627e-02, -1.6110e-02, -1.3451e-02,\n",
      "        -2.2928e-02,  2.8704e-02,  3.2897e-02, -1.8344e-02, -5.3425e-03,\n",
      "         3.6854e-02, -4.2129e-02, -4.1155e-02,  8.6251e-03,  2.4870e-02,\n",
      "         1.8528e-02, -1.0484e-02, -1.5819e-02,  1.4903e-03,  2.0253e-02,\n",
      "         4.1907e-02,  5.4209e-02,  3.6942e-02,  2.2821e-03, -4.2216e-03,\n",
      "         3.8692e-02,  9.8175e-03, -8.4452e-03,  1.3811e-02, -3.8086e-02,\n",
      "         3.5227e-02,  2.0999e-02, -1.8728e-02, -1.4354e-02, -3.4454e-02,\n",
      "         6.3099e-03, -6.9888e-03, -1.9802e-02,  1.2908e-02,  7.1477e-02,\n",
      "         1.0663e-02, -1.1199e-02,  1.2510e-02, -9.1299e-03,  1.9121e-03,\n",
      "        -1.6977e-02, -9.5771e-03, -2.2669e-02,  1.1887e-02, -1.1069e-02,\n",
      "        -2.2401e-02,  5.6335e-03, -3.8107e-02, -3.0840e-02,  5.5438e-02,\n",
      "         1.0803e-03, -3.8146e-02,  2.1184e-02,  3.9913e-02,  4.7726e-03,\n",
      "         1.8806e-02, -2.4984e-02, -3.6154e-02, -3.6160e-02,  3.1554e-02,\n",
      "        -4.7552e-02, -5.7377e-03,  3.6337e-02, -2.9643e-02,  7.8395e-03,\n",
      "        -1.9636e-02,  9.3237e-03, -2.8329e-02, -2.5890e-02,  3.5829e-02,\n",
      "        -1.8075e-02,  1.8700e-02, -4.2304e-02, -2.6714e-02, -4.6344e-02,\n",
      "        -3.1819e-02, -3.2013e-02, -2.2238e-02,  3.1856e-03, -5.9213e-02,\n",
      "        -1.7935e-02,  2.9059e-02, -3.8469e-02, -3.6444e-02,  4.7077e-03,\n",
      "        -1.0668e-02, -7.9490e-03,  3.7911e-02, -7.5106e-03, -3.6768e-02,\n",
      "        -3.5971e-02,  4.5196e-03, -7.2669e-03,  1.8679e-02,  1.9078e-02,\n",
      "         6.0851e-03,  5.7367e-03,  2.8011e-02, -3.4142e-02, -2.2813e-02,\n",
      "         1.3272e-02,  5.9634e-04, -3.4816e-02, -5.5194e-02, -4.9509e-02,\n",
      "        -4.5388e-02, -8.5806e-03, -3.3093e-02,  3.5723e-02,  3.4602e-03,\n",
      "        -2.6407e-02, -1.2604e-02,  1.8896e-02,  1.4146e-02, -3.9582e-02,\n",
      "         1.5500e-02, -2.3103e-02,  9.3282e-03,  4.3117e-02, -3.9228e-02,\n",
      "        -2.2586e-03,  2.4921e-02,  2.8511e-03, -1.1851e-02, -3.1245e-02,\n",
      "         2.7378e-02,  3.0420e-02,  3.3651e-02,  8.5728e-03, -1.4301e-02,\n",
      "        -2.3884e-02, -6.1091e-03, -2.0051e-02,  1.2204e-02, -5.5735e-03,\n",
      "         2.0999e-02,  5.3228e-02,  5.5069e-02,  4.8568e-02, -1.0083e-02,\n",
      "         3.2680e-02, -3.9303e-02, -1.9289e-02, -1.1546e-02, -5.1931e-05,\n",
      "         2.9451e-02, -2.0450e-02, -1.1216e-02, -1.9400e-02,  2.1293e-02,\n",
      "         2.1463e-02,  2.9664e-02,  1.6790e-02,  5.0993e-02,  5.0428e-02,\n",
      "         9.8939e-03, -3.8106e-02,  4.4501e-03,  3.0023e-02, -3.5036e-02],\n",
      "       grad_fn=<AddBackward0>)\n",
      "All its senses: ['management.n.01', 'management.n.02']\n",
      "\n",
      "Word lemma: management\n",
      "Word sense embedding: tensor([ 2.5534e-02,  3.4708e-03,  2.3494e-02, -7.6445e-04,  8.4899e-04,\n",
      "         2.2542e-02,  1.1691e-03,  8.4082e-04,  1.8722e-02,  1.6474e-02,\n",
      "         2.5857e-02,  1.0272e-02,  2.3435e-02, -3.5806e-02, -2.1796e-02,\n",
      "         3.3609e-02,  1.0061e-03,  4.7639e-02, -3.4437e-02, -2.4222e-02,\n",
      "         3.4464e-02, -3.7369e-03,  2.7371e-02, -5.4055e-02,  2.2487e-02,\n",
      "        -5.2070e-02, -3.6508e-02,  3.1559e-02,  1.8386e-02,  5.9208e-03,\n",
      "         1.2966e-02, -3.5360e-03,  4.5277e-02, -2.0263e-02,  5.2364e-03,\n",
      "         3.6781e-02, -4.7464e-02,  4.9241e-04,  2.0630e-02, -3.3964e-02,\n",
      "        -1.4893e-02,  5.4309e-02,  1.5966e-02, -1.9192e-02,  6.3491e-02,\n",
      "        -2.6885e-02, -1.2124e-02,  5.6685e-02,  1.6473e-02, -1.8161e-02,\n",
      "         2.4216e-02, -3.0243e-02,  2.6760e-02,  2.1545e-02,  1.1219e-02,\n",
      "        -1.1798e-02,  7.9294e-03, -2.4335e-02, -2.0969e-02, -5.5773e-03,\n",
      "         5.3417e-02,  2.1558e-02,  1.7595e-03, -5.8896e-02, -9.5363e-03,\n",
      "         4.4723e-02,  1.0212e-02, -1.5940e-02,  1.0538e-02,  4.2645e-02,\n",
      "        -1.5014e-02,  3.9265e-02, -3.0750e-02, -2.8170e-02, -2.0024e-02,\n",
      "        -2.1138e-03,  8.7330e-03,  1.5073e-02,  2.5225e-02, -1.1360e-02,\n",
      "        -1.9141e-03, -2.6477e-02, -2.2589e-03,  1.7649e-02,  4.0428e-02,\n",
      "         1.4371e-03, -8.2025e-03, -3.2969e-02,  7.4048e-02, -2.0435e-02,\n",
      "        -2.6900e-02, -4.8973e-03, -1.6035e-02, -1.1252e-03, -1.9425e-02,\n",
      "        -4.3546e-02,  9.7386e-03,  5.3575e-02,  2.2181e-02,  2.7515e-02,\n",
      "         4.0822e-02, -8.8872e-03, -4.3745e-02, -1.3805e-02,  1.5397e-02,\n",
      "        -3.1757e-02, -2.6561e-02, -8.2037e-03,  3.6314e-05,  4.7801e-02,\n",
      "         2.5834e-02,  3.8506e-02,  4.4954e-02,  6.2096e-03,  4.3167e-03,\n",
      "         3.5138e-02, -5.6427e-02,  1.1808e-03,  1.8212e-02,  5.4590e-02,\n",
      "         3.4851e-02, -1.5258e-02,  2.8995e-02, -2.2173e-02,  1.7633e-02,\n",
      "         2.6184e-02,  2.2648e-02,  2.5213e-02, -1.5948e-02, -1.8558e-03,\n",
      "        -1.0893e-02,  4.5293e-02, -3.5017e-02,  2.9594e-02,  2.0435e-03,\n",
      "         1.2927e-02, -1.7632e-02,  2.3627e-02, -1.6110e-02, -1.3451e-02,\n",
      "        -2.2928e-02,  2.8704e-02,  3.2897e-02, -1.8344e-02, -5.3425e-03,\n",
      "         3.6854e-02, -4.2129e-02, -4.1155e-02,  8.6251e-03,  2.4870e-02,\n",
      "         1.8528e-02, -1.0484e-02, -1.5819e-02,  1.4903e-03,  2.0253e-02,\n",
      "         4.1907e-02,  5.4209e-02,  3.6942e-02,  2.2821e-03, -4.2216e-03,\n",
      "         3.8692e-02,  9.8175e-03, -8.4452e-03,  1.3811e-02, -3.8086e-02,\n",
      "         3.5227e-02,  2.0999e-02, -1.8728e-02, -1.4354e-02, -3.4454e-02,\n",
      "         6.3099e-03, -6.9888e-03, -1.9802e-02,  1.2908e-02,  7.1477e-02,\n",
      "         1.0663e-02, -1.1199e-02,  1.2510e-02, -9.1299e-03,  1.9121e-03,\n",
      "        -1.6977e-02, -9.5771e-03, -2.2669e-02,  1.1887e-02, -1.1069e-02,\n",
      "        -2.2401e-02,  5.6335e-03, -3.8107e-02, -3.0840e-02,  5.5438e-02,\n",
      "         1.0803e-03, -3.8146e-02,  2.1184e-02,  3.9913e-02,  4.7726e-03,\n",
      "         1.8806e-02, -2.4984e-02, -3.6154e-02, -3.6160e-02,  3.1554e-02,\n",
      "        -4.7552e-02, -5.7377e-03,  3.6337e-02, -2.9643e-02,  7.8395e-03,\n",
      "        -1.9636e-02,  9.3237e-03, -2.8329e-02, -2.5890e-02,  3.5829e-02,\n",
      "        -1.8075e-02,  1.8700e-02, -4.2304e-02, -2.6714e-02, -4.6344e-02,\n",
      "        -3.1819e-02, -3.2013e-02, -2.2238e-02,  3.1856e-03, -5.9213e-02,\n",
      "        -1.7935e-02,  2.9059e-02, -3.8469e-02, -3.6444e-02,  4.7077e-03,\n",
      "        -1.0668e-02, -7.9490e-03,  3.7911e-02, -7.5106e-03, -3.6768e-02,\n",
      "        -3.5971e-02,  4.5196e-03, -7.2669e-03,  1.8679e-02,  1.9078e-02,\n",
      "         6.0851e-03,  5.7367e-03,  2.8011e-02, -3.4142e-02, -2.2813e-02,\n",
      "         1.3272e-02,  5.9634e-04, -3.4816e-02, -5.5194e-02, -4.9509e-02,\n",
      "        -4.5388e-02, -8.5806e-03, -3.3093e-02,  3.5723e-02,  3.4602e-03,\n",
      "        -2.6407e-02, -1.2604e-02,  1.8896e-02,  1.4146e-02, -3.9582e-02,\n",
      "         1.5500e-02, -2.3103e-02,  9.3282e-03,  4.3117e-02, -3.9228e-02,\n",
      "        -2.2586e-03,  2.4921e-02,  2.8511e-03, -1.1851e-02, -3.1245e-02,\n",
      "         2.7378e-02,  3.0420e-02,  3.3651e-02,  8.5728e-03, -1.4301e-02,\n",
      "        -2.3884e-02, -6.1091e-03, -2.0051e-02,  1.2204e-02, -5.5735e-03,\n",
      "         2.0999e-02,  5.3228e-02,  5.5069e-02,  4.8568e-02, -1.0083e-02,\n",
      "         3.2680e-02, -3.9303e-02, -1.9289e-02, -1.1546e-02, -5.1931e-05,\n",
      "         2.9451e-02, -2.0450e-02, -1.1216e-02, -1.9400e-02,  2.1293e-02,\n",
      "         2.1463e-02,  2.9664e-02,  1.6790e-02,  5.0993e-02,  5.0428e-02,\n",
      "         9.8939e-03, -3.8106e-02,  4.4501e-03,  3.0023e-02, -3.5036e-02],\n",
      "       grad_fn=<AddBackward0>)\n",
      "All its senses: ['management.n.01', 'management.n.02']\n",
      "\n",
      "Word lemma: house\n",
      "Word sense embedding: tensor([ 0.0248,  0.0056,  0.0249, -0.0098,  0.0166,  0.0101,  0.0210,  0.0031,\n",
      "         0.0124,  0.0242,  0.0286,  0.0115,  0.0358, -0.0390, -0.0292,  0.0329,\n",
      "        -0.0035,  0.0485, -0.0304, -0.0212,  0.0300, -0.0041,  0.0218, -0.0447,\n",
      "         0.0274, -0.0558, -0.0331,  0.0257,  0.0305,  0.0128,  0.0038,  0.0053,\n",
      "         0.0436, -0.0295,  0.0081,  0.0387, -0.0426,  0.0081,  0.0197, -0.0371,\n",
      "        -0.0254,  0.0531,  0.0140, -0.0217,  0.0716, -0.0284, -0.0102,  0.0594,\n",
      "         0.0222, -0.0204,  0.0259, -0.0253,  0.0331,  0.0253,  0.0037, -0.0042,\n",
      "         0.0058, -0.0268, -0.0208, -0.0173,  0.0503,  0.0171, -0.0085, -0.0564,\n",
      "        -0.0124,  0.0404,  0.0074, -0.0185,  0.0119,  0.0505, -0.0229,  0.0473,\n",
      "        -0.0223, -0.0167, -0.0141,  0.0043,  0.0111,  0.0124,  0.0223, -0.0025,\n",
      "         0.0010, -0.0222, -0.0082,  0.0237,  0.0370,  0.0022, -0.0076, -0.0277,\n",
      "         0.0608, -0.0165, -0.0345, -0.0151, -0.0191, -0.0126, -0.0229, -0.0508,\n",
      "         0.0150,  0.0573,  0.0285,  0.0296,  0.0299, -0.0070, -0.0486, -0.0129,\n",
      "         0.0143, -0.0298, -0.0290, -0.0109, -0.0011,  0.0461,  0.0266,  0.0378,\n",
      "         0.0284, -0.0003, -0.0037,  0.0435, -0.0561,  0.0101,  0.0170,  0.0570,\n",
      "         0.0408, -0.0090,  0.0178, -0.0257,  0.0158,  0.0225,  0.0215,  0.0170,\n",
      "        -0.0147, -0.0068, -0.0131,  0.0311, -0.0319,  0.0316, -0.0081,  0.0027,\n",
      "        -0.0134,  0.0337, -0.0195, -0.0194, -0.0184,  0.0306,  0.0311, -0.0134,\n",
      "        -0.0134,  0.0317, -0.0377, -0.0453, -0.0026,  0.0345,  0.0110, -0.0122,\n",
      "        -0.0188,  0.0038,  0.0291,  0.0514,  0.0472,  0.0368,  0.0150, -0.0060,\n",
      "         0.0295,  0.0109, -0.0036,  0.0133, -0.0411,  0.0300,  0.0213, -0.0221,\n",
      "        -0.0076, -0.0430,  0.0150, -0.0158, -0.0148,  0.0130,  0.0661,  0.0028,\n",
      "        -0.0099,  0.0211, -0.0114, -0.0010, -0.0131, -0.0113, -0.0115,  0.0223,\n",
      "        -0.0062, -0.0300,  0.0032, -0.0457, -0.0387,  0.0547, -0.0001, -0.0322,\n",
      "         0.0199,  0.0353,  0.0043,  0.0189, -0.0298, -0.0363, -0.0460,  0.0363,\n",
      "        -0.0501, -0.0033,  0.0407, -0.0220,  0.0182, -0.0151,  0.0078, -0.0382,\n",
      "        -0.0243,  0.0377, -0.0256,  0.0193, -0.0358, -0.0250, -0.0463, -0.0254,\n",
      "        -0.0268, -0.0104, -0.0012, -0.0436, -0.0174,  0.0257, -0.0401, -0.0312,\n",
      "         0.0035, -0.0038, -0.0070,  0.0400, -0.0114, -0.0421, -0.0285,  0.0045,\n",
      "        -0.0033,  0.0133,  0.0203,  0.0092,  0.0125,  0.0259, -0.0325, -0.0076,\n",
      "         0.0152,  0.0097, -0.0265, -0.0610, -0.0527, -0.0483, -0.0048, -0.0302,\n",
      "         0.0386,  0.0087, -0.0245, -0.0151,  0.0156,  0.0075, -0.0368,  0.0179,\n",
      "        -0.0216,  0.0050,  0.0370, -0.0377, -0.0038,  0.0215,  0.0021, -0.0085,\n",
      "        -0.0372,  0.0276,  0.0250,  0.0329,  0.0029, -0.0124, -0.0253,  0.0035,\n",
      "        -0.0312,  0.0190, -0.0001,  0.0138,  0.0529,  0.0539,  0.0533, -0.0106,\n",
      "         0.0391, -0.0333, -0.0119, -0.0222, -0.0102,  0.0221, -0.0197, -0.0108,\n",
      "        -0.0207,  0.0214,  0.0113,  0.0255,  0.0106,  0.0409,  0.0496,  0.0050,\n",
      "        -0.0368,  0.0048,  0.0171, -0.0464], grad_fn=<AddBackward0>)\n",
      "All its senses: ['house.n.01', 'firm.n.01', 'house.n.03', 'house.n.04', 'house.n.05', 'house.n.06', 'house.n.07', 'sign_of_the_zodiac.n.01', 'house.n.09', 'family.n.01', 'theater.n.01', 'house.n.12']\n",
      "\n",
      "Word lemma: house\n",
      "Word sense embedding: tensor([ 0.0248,  0.0056,  0.0249, -0.0098,  0.0166,  0.0101,  0.0210,  0.0031,\n",
      "         0.0124,  0.0242,  0.0286,  0.0115,  0.0358, -0.0390, -0.0292,  0.0329,\n",
      "        -0.0035,  0.0485, -0.0304, -0.0212,  0.0300, -0.0041,  0.0218, -0.0447,\n",
      "         0.0274, -0.0558, -0.0331,  0.0257,  0.0305,  0.0128,  0.0038,  0.0053,\n",
      "         0.0436, -0.0295,  0.0081,  0.0387, -0.0426,  0.0081,  0.0197, -0.0371,\n",
      "        -0.0254,  0.0531,  0.0140, -0.0217,  0.0716, -0.0284, -0.0102,  0.0594,\n",
      "         0.0222, -0.0204,  0.0259, -0.0253,  0.0331,  0.0253,  0.0037, -0.0042,\n",
      "         0.0058, -0.0268, -0.0208, -0.0173,  0.0503,  0.0171, -0.0085, -0.0564,\n",
      "        -0.0124,  0.0404,  0.0074, -0.0185,  0.0119,  0.0505, -0.0229,  0.0473,\n",
      "        -0.0223, -0.0167, -0.0141,  0.0043,  0.0111,  0.0124,  0.0223, -0.0025,\n",
      "         0.0010, -0.0222, -0.0082,  0.0237,  0.0370,  0.0022, -0.0076, -0.0277,\n",
      "         0.0608, -0.0165, -0.0345, -0.0151, -0.0191, -0.0126, -0.0229, -0.0508,\n",
      "         0.0150,  0.0573,  0.0285,  0.0296,  0.0299, -0.0070, -0.0486, -0.0129,\n",
      "         0.0143, -0.0298, -0.0290, -0.0109, -0.0011,  0.0461,  0.0266,  0.0378,\n",
      "         0.0284, -0.0003, -0.0037,  0.0435, -0.0561,  0.0101,  0.0170,  0.0570,\n",
      "         0.0408, -0.0090,  0.0178, -0.0257,  0.0158,  0.0225,  0.0215,  0.0170,\n",
      "        -0.0147, -0.0068, -0.0131,  0.0311, -0.0319,  0.0316, -0.0081,  0.0027,\n",
      "        -0.0134,  0.0337, -0.0195, -0.0194, -0.0184,  0.0306,  0.0311, -0.0134,\n",
      "        -0.0134,  0.0317, -0.0377, -0.0453, -0.0026,  0.0345,  0.0110, -0.0122,\n",
      "        -0.0188,  0.0038,  0.0291,  0.0514,  0.0472,  0.0368,  0.0150, -0.0060,\n",
      "         0.0295,  0.0109, -0.0036,  0.0133, -0.0411,  0.0300,  0.0213, -0.0221,\n",
      "        -0.0076, -0.0430,  0.0150, -0.0158, -0.0148,  0.0130,  0.0661,  0.0028,\n",
      "        -0.0099,  0.0211, -0.0114, -0.0010, -0.0131, -0.0113, -0.0115,  0.0223,\n",
      "        -0.0062, -0.0300,  0.0032, -0.0457, -0.0387,  0.0547, -0.0001, -0.0322,\n",
      "         0.0199,  0.0353,  0.0043,  0.0189, -0.0298, -0.0363, -0.0460,  0.0363,\n",
      "        -0.0501, -0.0033,  0.0407, -0.0220,  0.0182, -0.0151,  0.0078, -0.0382,\n",
      "        -0.0243,  0.0377, -0.0256,  0.0193, -0.0358, -0.0250, -0.0463, -0.0254,\n",
      "        -0.0268, -0.0104, -0.0012, -0.0436, -0.0174,  0.0257, -0.0401, -0.0312,\n",
      "         0.0035, -0.0038, -0.0070,  0.0400, -0.0114, -0.0421, -0.0285,  0.0045,\n",
      "        -0.0033,  0.0133,  0.0203,  0.0092,  0.0125,  0.0259, -0.0325, -0.0076,\n",
      "         0.0152,  0.0097, -0.0265, -0.0610, -0.0527, -0.0483, -0.0048, -0.0302,\n",
      "         0.0386,  0.0087, -0.0245, -0.0151,  0.0156,  0.0075, -0.0368,  0.0179,\n",
      "        -0.0216,  0.0050,  0.0370, -0.0377, -0.0038,  0.0215,  0.0021, -0.0085,\n",
      "        -0.0372,  0.0276,  0.0250,  0.0329,  0.0029, -0.0124, -0.0253,  0.0035,\n",
      "        -0.0312,  0.0190, -0.0001,  0.0138,  0.0529,  0.0539,  0.0533, -0.0106,\n",
      "         0.0391, -0.0333, -0.0119, -0.0222, -0.0102,  0.0221, -0.0197, -0.0108,\n",
      "        -0.0207,  0.0214,  0.0113,  0.0255,  0.0106,  0.0409,  0.0496,  0.0050,\n",
      "        -0.0368,  0.0048,  0.0171, -0.0464], grad_fn=<AddBackward0>)\n",
      "All its senses: ['house.n.01', 'firm.n.01', 'house.n.03', 'house.n.04', 'house.n.05', 'house.n.06', 'house.n.07', 'sign_of_the_zodiac.n.01', 'house.n.09', 'family.n.01', 'theater.n.01', 'house.n.12']\n",
      "\n",
      "Word lemma: overthrow\n",
      "Word sense embedding: tensor([ 1.0168e-02,  4.2319e-03,  2.2583e-02,  1.0991e-02,  1.7244e-02,\n",
      "         1.4031e-02,  1.1423e-02, -6.3874e-03,  2.6994e-02,  3.0097e-02,\n",
      "         2.5939e-02,  7.3562e-03,  2.4620e-02, -4.3698e-02, -2.1954e-02,\n",
      "         3.2844e-02,  4.9322e-03,  3.6575e-02, -2.8604e-02, -2.4424e-02,\n",
      "         3.3828e-02, -1.9586e-02,  2.8495e-02, -4.6875e-02,  1.0752e-02,\n",
      "        -4.5848e-02, -3.2077e-02,  2.3552e-02,  2.6178e-02,  2.6253e-03,\n",
      "         8.6655e-03, -3.2792e-03,  3.1103e-02, -2.6205e-02,  8.4851e-03,\n",
      "         3.3838e-02, -4.2981e-02, -3.4620e-04,  2.2655e-02, -2.9207e-02,\n",
      "        -2.4410e-02,  4.7021e-02,  8.4293e-03, -2.7492e-02,  6.7598e-02,\n",
      "        -3.1229e-02, -1.9222e-02,  5.8688e-02,  2.4020e-02, -2.3331e-02,\n",
      "         2.6090e-02, -3.0296e-02,  2.0736e-02,  2.9981e-02,  3.5303e-03,\n",
      "        -1.7724e-02,  1.0189e-02, -4.3093e-02, -1.3146e-02, -1.0093e-02,\n",
      "         4.4669e-02,  1.6289e-02,  2.4090e-03, -6.1441e-02, -9.9808e-03,\n",
      "         4.3922e-02,  1.0175e-02, -1.9317e-02,  2.7806e-02,  3.5102e-02,\n",
      "        -2.0475e-02,  5.3331e-02, -3.6259e-02, -2.1856e-02, -1.1643e-02,\n",
      "         5.2460e-03,  1.0362e-02,  1.2384e-02,  2.7427e-02, -2.9434e-03,\n",
      "        -4.3929e-03, -2.9064e-02,  8.2204e-03,  1.2366e-02,  3.4075e-02,\n",
      "        -4.4649e-05, -1.0741e-02, -3.5128e-02,  7.6244e-02, -2.2054e-02,\n",
      "        -3.4186e-02, -1.1888e-02, -1.8996e-02, -1.0666e-02, -2.5620e-02,\n",
      "        -4.4431e-02, -3.7156e-03,  5.1910e-02,  3.0691e-02,  2.5186e-02,\n",
      "         3.4252e-02, -3.4508e-03, -4.3525e-02, -1.6586e-02,  1.1058e-02,\n",
      "        -4.4277e-02, -3.6226e-02, -6.2777e-03,  2.6667e-03,  4.2517e-02,\n",
      "         2.9386e-02,  2.6260e-02,  4.1034e-02,  1.0969e-03,  7.8838e-03,\n",
      "         3.3277e-02, -4.7748e-02,  4.2368e-03,  2.2183e-02,  5.2275e-02,\n",
      "         3.1296e-02, -1.3352e-02,  1.7557e-02, -2.0427e-02,  1.8244e-02,\n",
      "         2.4455e-02,  2.2776e-02,  3.2547e-02, -1.0298e-02, -1.8049e-02,\n",
      "        -4.9015e-04,  3.4851e-02, -3.6182e-02,  3.3777e-02,  3.9057e-03,\n",
      "         6.9677e-03, -1.1635e-02,  2.9180e-02, -1.5218e-02,  2.5188e-03,\n",
      "        -2.2146e-02,  3.1094e-02,  3.6551e-02, -2.0426e-02, -1.5022e-02,\n",
      "         3.5499e-02, -3.5650e-02, -5.1595e-02,  2.5662e-03,  3.7911e-02,\n",
      "         7.3380e-03, -5.0912e-03, -6.8366e-03,  1.5942e-03,  1.0360e-02,\n",
      "         3.1525e-02,  5.2287e-02,  4.0295e-02,  1.0422e-02,  1.2086e-03,\n",
      "         4.0244e-02,  2.0388e-02, -1.2619e-02,  2.1246e-02, -3.0357e-02,\n",
      "         3.5770e-02,  1.7296e-02, -2.2256e-02, -1.2404e-02, -4.1279e-02,\n",
      "         8.6964e-03, -1.7086e-02, -1.5024e-02,  9.5637e-03,  7.3639e-02,\n",
      "         5.3737e-03, -1.2957e-02,  1.1077e-02,  1.6431e-03,  3.1888e-03,\n",
      "        -1.0708e-02, -1.1593e-02, -1.0607e-02,  2.4407e-02, -1.8032e-03,\n",
      "        -2.2360e-02, -8.9306e-04, -4.2536e-02, -3.9005e-02,  5.7179e-02,\n",
      "        -7.8471e-03, -3.6770e-02,  1.9810e-02,  3.4471e-02, -8.4080e-03,\n",
      "         2.3887e-02, -2.1492e-02, -4.0324e-02, -4.5375e-02,  2.9030e-02,\n",
      "        -4.9914e-02,  6.7098e-03,  3.4500e-02, -2.9891e-02,  2.3749e-02,\n",
      "        -1.8424e-02,  2.5192e-02, -3.1749e-02, -2.1315e-02,  3.4437e-02,\n",
      "        -2.4897e-02,  1.8164e-02, -3.9264e-02, -3.1134e-02, -4.5518e-02,\n",
      "        -2.7945e-02, -3.8067e-02, -2.1060e-02, -7.8419e-03, -4.5461e-02,\n",
      "        -1.0967e-02,  3.3357e-02, -4.6185e-02, -3.4187e-02,  3.1089e-03,\n",
      "        -1.1072e-02, -1.3035e-02,  4.4153e-02, -9.6446e-03, -4.5575e-02,\n",
      "        -3.3669e-02,  9.3905e-03,  1.5317e-03,  1.5356e-02,  1.6257e-02,\n",
      "         1.0416e-02,  1.4078e-02,  2.7364e-02, -2.8477e-02, -1.6812e-02,\n",
      "         1.3155e-02,  1.7812e-03, -2.7371e-02, -5.7603e-02, -5.1465e-02,\n",
      "        -5.3243e-02,  7.0761e-04, -3.1592e-02,  2.5778e-02,  3.7740e-03,\n",
      "        -2.5394e-02, -1.1844e-02,  2.5167e-02,  1.8444e-02, -3.5622e-02,\n",
      "         2.5105e-02, -2.5819e-02,  4.0139e-03,  4.2979e-02, -4.6406e-02,\n",
      "         2.2580e-03,  2.6802e-02,  1.6113e-03, -1.8418e-02, -3.2012e-02,\n",
      "         3.0543e-02,  2.3551e-02,  4.1427e-02,  5.2610e-03, -1.3427e-02,\n",
      "        -1.5405e-02, -8.2667e-03, -3.2923e-02,  1.7619e-02, -5.7004e-03,\n",
      "         1.9445e-02,  4.4952e-02,  4.7280e-02,  4.0478e-02, -1.2888e-02,\n",
      "         4.3539e-02, -2.5381e-02, -1.4138e-02, -7.6382e-03,  3.1921e-03,\n",
      "         3.2164e-02, -2.4055e-02, -1.4034e-02, -1.3931e-02,  2.3950e-02,\n",
      "         2.0973e-02,  2.1292e-02,  1.8269e-02,  4.8605e-02,  3.5643e-02,\n",
      "         1.0534e-02, -3.5002e-02,  7.4087e-03,  1.7068e-02, -3.6655e-02],\n",
      "       grad_fn=<AddBackward0>)\n",
      "All its senses: ['overthrow.n.01', 'upset.n.02']\n"
     ]
    }
   ],
   "source": [
    "# forward propagation\n",
    "# ELMo (1024) -> dimension reduction (256) -> bi-LSTM (512) -> fine-tuning MLP (10)\n",
    "results_45 = model.forward(train_X, train_word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
