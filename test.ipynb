{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import string\n",
    "import itertools\n",
    "from io import open\n",
    "from conllu import parse_incr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the WSD dataset first\n",
    "\n",
    "'''\n",
    "Copyright@\n",
    "White, A. S., D. Reisinger, K. Sakaguchi, T. Vieira, S. Zhang, R. Rudinger, K. Rawlins, & B. Van Durme. 2016. \n",
    "[Universal decompositional semantics on universal dependencies]\n",
    "(http://aswhite.net/media/papers/white_universal_2016.pdf). \n",
    "To appear in *Proceedings of the Conference on Empirical Methods in Natural Language Processing 2016*.\n",
    "'''\n",
    "\n",
    "def parse_wsd_data():\n",
    "\n",
    "    # parse the WSD dataset\n",
    "    wsd_data = []\n",
    "\n",
    "    # read in tsv by White et. al., 2016\n",
    "    with open('data/wsd/wsd_eng_ud1.2_10262016.tsv', mode = 'r') as wsd_file:\n",
    "\n",
    "        tsv_reader = csv.DictReader(wsd_file, delimiter = '\\t')      \n",
    "\n",
    "        # store the data: ordered dict row\n",
    "        for row in tsv_reader:                                \n",
    "\n",
    "            # each data vector\n",
    "            wsd_data.append(row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print('Parsed {} word sense data from White et. al., 2016.'.format(len(wsd_data)))\n",
    "\n",
    "    return wsd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 439312 word sense data from White et. al., 2016.\n"
     ]
    }
   ],
   "source": [
    "# get the raw wsd data\n",
    "wsd_data = parse_wsd_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "return: \n",
    "all senses for each word \n",
    "all definitions for each word\n",
    "all supersenses\n",
    "from the EUD for train, test, and dev dataset\n",
    "index provided by WSD dataset by White et. al.\n",
    "'''\n",
    "# get all the senses and definitions for each word from WSD dataset\n",
    "# order of senses and definitions are in order\n",
    "def get_all_senses_and_definitions(wsd_data):\n",
    "\n",
    "    # all senses for each word in train and dev\n",
    "    # supersense is shared \n",
    "    all_senses = {}\n",
    "    all_definitions = {}\n",
    "    all_supersenses = {}\n",
    "\n",
    "    # all senses for each word in test\n",
    "    all_test_senses = {}\n",
    "    all_test_definitions = {}\n",
    "    \n",
    "    # only get the senses for train and dev set\n",
    "    for i in range(len(wsd_data)):\n",
    "        \n",
    "        # get the original sentence from EUD\n",
    "        sentence_id = wsd_data[i].get('Sentence.ID')\n",
    "        \n",
    "        # get the definitions for the target word from EUD\n",
    "        definition = wsd_data[i].get('Sense.Definition').split(' ')\n",
    "        \n",
    "        # the index in EUD is 1-based!!!\n",
    "        sentence_number = int(sentence_id.split(' ')[-1]) - 1\n",
    "        word_index = int(wsd_data[i].get('Arg.Token')) - 1\n",
    "        word_lemma = wsd_data[i].get('Arg.Lemma')\n",
    "        word_sense = wsd_data[i].get('Synset')\n",
    "        response = wsd_data[i].get('Sense.Response')\n",
    "        \n",
    "        # senses for train and dev\n",
    "        # preserve unknown words\n",
    "        if wsd_data[i].get('Split') != 'test':\n",
    "        \n",
    "            # supersense-> (word_lemma, word_sense) dictionary\n",
    "            super_s = wn.synset(word_sense).lexname().replace('.', '_')\n",
    "            if all_supersenses.get(super_s, 'not_exist') != 'not_exist':\n",
    "                all_supersenses[super_s].add((word_lemma, word_sense))\n",
    "            else:\n",
    "                all_supersenses[super_s] = {(word_lemma, word_sense)}\n",
    "\n",
    "            # if the word already exits: add the new sense to the list\n",
    "            # else: creata a new list for the word\n",
    "            if all_senses.get(word_lemma, 'not_exist') != 'not_exist':\n",
    "                if word_sense not in all_senses[word_lemma]:\n",
    "                    all_senses[word_lemma].append(word_sense)\n",
    "            else:\n",
    "                all_senses[word_lemma] = []\n",
    "                all_senses[word_lemma].append(word_sense)            \n",
    "            \n",
    "            if all_definitions.get(word_lemma,'not_exist') != 'not_exist':\n",
    "                if definition not in all_definitions[word_lemma]: \n",
    "                    all_definitions[word_lemma].append(definition)\n",
    "            else:\n",
    "                all_definitions[word_lemma] = []\n",
    "                all_definitions[word_lemma].append(definition)\n",
    "                \n",
    "        else:\n",
    "\n",
    "            # all the senses and definitions for test words\n",
    "            if all_test_senses.get(word_lemma, 'not_exist') != 'not_exist':\n",
    "                if word_sense not in all_test_senses[word_lemma]:\n",
    "                    all_test_senses[word_lemma].append(word_sense)\n",
    "            else:\n",
    "                all_test_senses[word_lemma] = []\n",
    "                all_test_senses[word_lemma].append(word_sense)            \n",
    "            \n",
    "            if all_test_definitions.get(word_lemma,'not_exist') != 'not_exist':\n",
    "                if definition not in all_test_definitions[word_lemma]: \n",
    "                    all_test_definitions[word_lemma].append(definition)\n",
    "            else:\n",
    "                all_test_definitions[word_lemma] = []\n",
    "                all_test_definitions[word_lemma].append(definition)            \n",
    "        \n",
    "    return all_senses, all_definitions, all_supersenses, all_test_senses, all_test_definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the senses and definitions\n",
    "all_senses, all_definitions, all_supersenses, all_test_senses, all_test_definitions = get_all_senses_and_definitions(wsd_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe specific Synset method is lexname, e.g. wn.synsets('spring')[0].lexname(). \\nThat should make it really easy to get the suspersenses.\\nAnd if you have the synset name–e.g. 'spring.n.01'\\nyou can access the supersense directly: wn.synset('spring.n.01').lexname().\\nWhich returns 'noun.time'.\\nAnd wn.synset('spring.n.02').lexname() returns 'noun.artifact'\\n\\nfor idx, d in enumerate(all_definitions['spring']):\\n    print(d)\\n    print(wn.synset(all_senses['spring'][idx]).lexname())\\n\\nfor _ in wn.synsets('spring'):\\n    print(_.lexname())\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test for the WordNet NLTK API\n",
    "'''\n",
    "The specific Synset method is lexname, e.g. wn.synsets('spring')[0].lexname(). \n",
    "That should make it really easy to get the suspersenses.\n",
    "And if you have the synset name–e.g. 'spring.n.01'\n",
    "you can access the supersense directly: wn.synset('spring.n.01').lexname().\n",
    "Which returns 'noun.time'.\n",
    "And wn.synset('spring.n.02').lexname() returns 'noun.artifact'\n",
    "\n",
    "for idx, d in enumerate(all_definitions['spring']):\n",
    "    print(d)\n",
    "    print(wn.synset(all_senses['spring'][idx]).lexname())\n",
    "\n",
    "for _ in wn.synsets('spring'):\n",
    "    print(_.lexname())\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train, dev, test datasets from processed files\n",
    "# check the 'data_loader.ipynb' for details\n",
    "def read_file():\n",
    "    \n",
    "    train_X = []\n",
    "    train_X_num = 0\n",
    "    train_Y = []\n",
    "    train_Y_num = 0\n",
    "    test_X = []\n",
    "    test_X_num = 0\n",
    "    test_Y = []\n",
    "    test_Y_num = 0\n",
    "    dev_X = []\n",
    "    dev_X_num = 0\n",
    "    dev_Y = []\n",
    "    dev_Y_num = 0\n",
    "    \n",
    "    train_word_idx = []\n",
    "    test_word_idx = []\n",
    "    dev_word_idx = []\n",
    "    \n",
    "    # read in csv\n",
    "    with open('data/train_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            train_X.append(row)\n",
    "            train_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {train_X_num} data points for train_X.')\n",
    "\n",
    "    with open('data/train_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            train_Y.append(row)\n",
    "            train_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {train_Y_num} data points for train_Y.')\n",
    "        \n",
    "    with open('data/train_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            train_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(train_word_idx)} data points for train_word_idx.')\n",
    "\n",
    "    with open('data/dev_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            dev_X.append(row)\n",
    "            dev_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {dev_X_num} data points for dev_X.')\n",
    "\n",
    "    with open('data/dev_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            dev_Y.append(row)\n",
    "            dev_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {dev_Y_num} data points for dev_Y.')\n",
    "        \n",
    "    with open('data/dev_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            dev_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(dev_word_idx)} data points for dev_word_idx.')\n",
    "        \n",
    "    with open('data/test_X.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file, delimiter = '\\t')\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            test_X.append(row)\n",
    "            test_X_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {test_X_num} data points for test_X.')\n",
    "\n",
    "    with open('data/test_Y.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            test_Y.append(row)\n",
    "            test_Y_num += 1\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {test_Y_num} data points for test_Y.')\n",
    "        \n",
    "    with open('data/test_word_idx.tsv', mode = 'r') as data_file:\n",
    "        \n",
    "        csv_reader = csv.reader(data_file)\n",
    "\n",
    "        # store the data\n",
    "        for row in csv_reader:\n",
    "\n",
    "            row = list(map(int, row))\n",
    "            test_word_idx = (row)\n",
    "\n",
    "        # make sure all data are parsed\n",
    "        print(f'Parsed {len(test_word_idx)} data points for test_word_idx.')    \n",
    "        \n",
    "    return train_X, train_Y, test_X, test_Y, dev_X, dev_Y, train_word_idx, test_word_idx, dev_word_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 67398 data points for train_X.\n",
      "Parsed 67398 data points for train_Y.\n",
      "Parsed 67398 data points for train_word_idx.\n",
      "Parsed 7333 data points for dev_X.\n",
      "Parsed 7333 data points for dev_Y.\n",
      "Parsed 7333 data points for dev_word_idx.\n",
      "Parsed 7123 data points for test_X.\n",
      "Parsed 7123 data points for test_Y.\n",
      "Parsed 7123 data points for test_word_idx.\n",
      "distri of train: [35. 43. 27. 11.  6.  3.  7.  5.]\n",
      "distri of test: [5. 5. 4. 1. 1. 1. 2. 1.]\n",
      "distri of dev: [4. 3. 0. 2. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# get all the structured data\n",
    "train_X, train_Y, test_X, test_Y, dev_X, dev_Y, train_word_idx, test_word_idx, dev_word_idx = read_file()\n",
    "\n",
    "# test on one word\n",
    "word_choice = 'level'\n",
    "\n",
    "new_train_X = []\n",
    "new_train_Y = []\n",
    "new_train_idx = []\n",
    "distri_train = np.zeros(len(all_test_senses[word_choice]))\n",
    "# stst = 0\n",
    "for index, sen in enumerate(train_X):\n",
    "    \n",
    "    if sen[train_word_idx[index]] == word_choice:\n",
    "        new_train_idx.append(train_word_idx[index])\n",
    "        new_train_X.append(sen)\n",
    "        new_train_Y.append(train_Y[index])\n",
    "        distri_train += np.asarray(train_Y[index])\n",
    "        # summ = train_Y[index][0] + train_Y[index][1]\n",
    "        # if summ != 2:\n",
    "            # stst += 1\n",
    "# print('stst: {}'.format(stst))        \n",
    "print('distri of train: {}'.format(distri_train))\n",
    "        \n",
    "new_test_X = []\n",
    "new_test_Y = []\n",
    "new_test_idx = []\n",
    "distri_test = np.zeros(len(all_test_senses[word_choice]))\n",
    "for index, sen in enumerate(test_X):\n",
    "    \n",
    "    if sen[test_word_idx[index]] == word_choice:\n",
    "        new_test_idx.append(test_word_idx[index])\n",
    "        new_test_X.append(sen)\n",
    "        new_test_Y.append(test_Y[index])\n",
    "        # print(test_Y[index])\n",
    "        distri_test += np.asarray(test_Y[index])\n",
    "print('distri of test: {}'.format(distri_test))\n",
    "\n",
    "new_dev_X = []\n",
    "new_dev_Y = []\n",
    "new_dev_idx = []\n",
    "distri_dev = np.zeros(len(all_test_senses[word_choice]))\n",
    "for index, sen in enumerate(dev_X):\n",
    "        \n",
    "    if sen[dev_word_idx[index]] == word_choice:\n",
    "        new_dev_idx.append(dev_word_idx[index])\n",
    "        new_dev_X.append(sen)\n",
    "        new_dev_Y.append(dev_Y[index])\n",
    "        distri_dev += np.asarray(dev_Y[index])\n",
    "print('distri of dev: {}'.format(distri_dev))\n",
    "\n",
    "target_senses = all_senses[word_choice]\n",
    "new_all_senses = {word_choice : target_senses}\n",
    "target_def = all_definitions[word_choice]\n",
    "new_all_def = {word_choice : target_def}\n",
    "\n",
    "# limit the supersense to only the test word\n",
    "new_all_supersenses = {}\n",
    "for supersense in all_supersenses.keys():\n",
    "    for tuples in all_supersenses[supersense]:\n",
    "        \n",
    "        if tuples[0] == word_choice:\n",
    "            if new_all_supersenses.get(supersense, 'e') != 'e':\n",
    "                new_all_supersenses[supersense].add((word_choice, tuples[1]))\n",
    "            else:\n",
    "                new_all_supersenses[supersense] = {(word_choice, tuples[1])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n",
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "from model import *\n",
    "from trainer import *\n",
    "\n",
    "from allennlp.commands.elmo import ElmoEmbedder\n",
    "elmo = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer\n",
    "epochs = 30\n",
    "\n",
    "# test on one word\n",
    "# trainer = Trainer(epochs = epochs, elmo_class = elmo, all_senses = all_senses, all_supersenses = all_supersenses)\n",
    "trainer = Trainer(epochs = epochs, elmo_class = elmo, all_senses = new_all_senses, all_supersenses = new_all_supersenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############   Model Parameters   ##############\n",
      "layers.word_sense.0.weight torch.Size([512, 512])\n",
      "layers.word_sense.0.bias torch.Size([512])\n",
      "layers.word_sense.2.weight torch.Size([300, 512])\n",
      "layers.word_sense.2.bias torch.Size([300])\n",
      "dimension_reduction_MLP.weight torch.Size([256, 3072])\n",
      "dimension_reduction_MLP.bias torch.Size([256])\n",
      "wsd_lstm.weight_ih_l0 torch.Size([1024, 256])\n",
      "wsd_lstm.weight_hh_l0 torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l0 torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l0 torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l0_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.weight_hh_l0_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l0_reverse torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l0_reverse torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l1 torch.Size([1024, 512])\n",
      "wsd_lstm.weight_hh_l1 torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l1 torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l1 torch.Size([1024])\n",
      "wsd_lstm.weight_ih_l1_reverse torch.Size([1024, 512])\n",
      "wsd_lstm.weight_hh_l1_reverse torch.Size([1024, 256])\n",
      "wsd_lstm.bias_ih_l1_reverse torch.Size([1024])\n",
      "wsd_lstm.bias_hh_l1_reverse torch.Size([1024])\n",
      "definition_embeddings.level torch.Size([300, 8])\n",
      "supersense_embeddings.noun_artifact torch.Size([300, 1])\n",
      "supersense_embeddings.noun_attribute torch.Size([300, 1])\n",
      "supersense_embeddings.noun_cognition torch.Size([300, 1])\n",
      "supersense_embeddings.noun_state torch.Size([300, 1])\n",
      "##################################################\n",
      "[Epoch: 1/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b61ffd1b4754582ab1b69289124ef46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1, Mean Training Loss: 13.065631866455078\n",
      "Epoch: 1, Mean Dev Loss: 8.860952241080147\n",
      "[Epoch: 2/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08760d79a7224ae69ed9ee86646758ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2, Mean Training Loss: 13.06666651539419\n",
      "Epoch: 2, Mean Dev Loss: 8.900476319449288\n",
      "[Epoch: 3/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c4f9e161414785b0a9df7c21d4abde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3, Mean Training Loss: 13.04030648593245\n",
      "Epoch: 3, Mean Dev Loss: 8.902856418064662\n",
      "[Epoch: 4/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44dd419da5764c1192590c1228d6c9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4, Mean Training Loss: 13.026627946174008\n",
      "Epoch: 4, Mean Dev Loss: 8.928094591413226\n",
      "[Epoch: 5/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cf72786b70e42c8bf08c3dc949d93f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5, Mean Training Loss: 13.064022294406232\n",
      "Epoch: 5, Mean Dev Loss: 8.91047613961356\n",
      "[Epoch: 6/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b765d668edd048a1b5862844571c33d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6, Mean Training Loss: 13.005900207607226\n",
      "Epoch: 6, Mean Dev Loss: 8.839999607631139\n",
      "[Epoch: 7/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4714f392b3e84fb392f58c2d51f25481",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7, Mean Training Loss: 12.96130292168979\n",
      "Epoch: 7, Mean Dev Loss: 8.78238024030413\n",
      "[Epoch: 8/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a36dd2b9a174c2db2ef261bf2b46bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8, Mean Training Loss: 12.93183764095964\n",
      "Epoch: 8, Mean Dev Loss: 8.803333009992327\n",
      "[Epoch: 9/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ccab67135d410a9081e185d96217b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9, Mean Training Loss: 12.92544060191889\n",
      "Epoch: 9, Mean Dev Loss: 8.796659060886928\n",
      "[Epoch: 10/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c56ba443bb841ec9c3864e72e96b11f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10, Mean Training Loss: 12.920267718961869\n",
      "Epoch: 10, Mean Dev Loss: 8.746190616062709\n",
      "[Epoch: 11/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e4b41a447ee4282aeb11fafbd530cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 11, Mean Training Loss: 12.880305958890366\n",
      "Epoch: 11, Mean Dev Loss: 8.691903931753975\n",
      "[Epoch: 12/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2dd30b1e6a44acb39b74ba5bd629c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 12, Mean Training Loss: 12.874137275520413\n",
      "Epoch: 12, Mean Dev Loss: 8.706665856497628\n",
      "[Epoch: 13/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d26d44bbd144eaaa959c979e57e949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 13, Mean Training Loss: 12.874520575863192\n",
      "Epoch: 13, Mean Dev Loss: 8.699047224862236\n",
      "[Epoch: 14/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13f5ce844d134412be7a257ec72ec599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 14, Mean Training Loss: 12.882834116617838\n",
      "Epoch: 14, Mean Dev Loss: 8.73952307019915\n",
      "[Epoch: 15/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fdc5c9508f140268e0523023257eccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 15, Mean Training Loss: 12.889348512408377\n",
      "Epoch: 15, Mean Dev Loss: 8.722856385367256\n",
      "[Epoch: 16/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b5199686e96461eaf3a25d177ad2f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 16, Mean Training Loss: 12.868811190813437\n",
      "Epoch: 16, Mean Dev Loss: 8.708095005580358\n",
      "[Epoch: 17/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e785413910124850b1f252d1fb5831dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 17, Mean Training Loss: 12.849233397122088\n",
      "Epoch: 17, Mean Dev Loss: 8.71952383858817\n",
      "[Epoch: 18/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dc3cd4b9a42483d814c570e67e62967",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 18, Mean Training Loss: 12.845516939272827\n",
      "Epoch: 18, Mean Dev Loss: 8.704761777605329\n",
      "[Epoch: 19/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4493ccd18564758be94811336a3902b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 19, Mean Training Loss: 12.843140339029246\n",
      "Epoch: 19, Mean Dev Loss: 8.717619350978307\n",
      "[Epoch: 20/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49c93c576e0499894f7b7981e15f983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 20, Mean Training Loss: 12.842069921822384\n",
      "Epoch: 20, Mean Dev Loss: 8.718094825744629\n",
      "[Epoch: 21/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1aea9a02b243dc8d275e9595d5bdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 21, Mean Training Loss: 12.819846821927475\n",
      "Epoch: 21, Mean Dev Loss: 8.73142841884068\n",
      "[Epoch: 22/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4aae7e4d00049c2bf4fac383e5d6e10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 22, Mean Training Loss: 12.805745946949926\n",
      "Epoch: 22, Mean Dev Loss: 8.72714260646275\n",
      "[Epoch: 23/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cac2ba99b95498f826f5eef67639306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 23, Mean Training Loss: 12.784137594288794\n",
      "Epoch: 23, Mean Dev Loss: 8.696666308811732\n",
      "[Epoch: 24/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "047283b7d3b14c0dbb2ff64138b60f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 24, Mean Training Loss: 12.78524977585365\n",
      "Epoch: 24, Mean Dev Loss: 8.684761864798409\n",
      "[Epoch: 25/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5752eca100a640b982d1cee76b6e1550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 25, Mean Training Loss: 12.793678053494158\n",
      "Epoch: 25, Mean Dev Loss: 8.67999976021903\n",
      "[Epoch: 26/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b16d5cf333941ccbe0a226011dba6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 26, Mean Training Loss: 12.785785346195615\n",
      "Epoch: 26, Mean Dev Loss: 8.66047545841762\n",
      "[Epoch: 27/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07c3e0206e844925aefb451cc168c195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 27, Mean Training Loss: 12.772643155065076\n",
      "Epoch: 27, Mean Dev Loss: 8.64142826625279\n",
      "[Epoch: 28/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8300ec2b2343e08b1f70d873554744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 28, Mean Training Loss: 12.750917829316238\n",
      "Epoch: 28, Mean Dev Loss: 8.637142726353236\n",
      "[Epoch: 29/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e5bb08f314041e283faf486df2afbc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 29, Mean Training Loss: 12.754404868202649\n",
      "Epoch: 29, Mean Dev Loss: 8.66047545841762\n",
      "[Epoch: 30/30]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c00e3aed1dd0485fb97a6b60caf670dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=87), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 30, Mean Training Loss: 12.757739089001184\n",
      "Epoch: 30, Mean Dev Loss: 8.653332710266113\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "# train_losses, dev_losses, dev_rs = trainer.train(train_X, train_Y, train_word_idx, dev_X, dev_Y, dev_word_idx)\n",
    "\n",
    "# small test on only one word\n",
    "train_losses, dev_losses, dev_rs = trainer.train(new_train_X, new_train_Y, new_train_idx, new_dev_X, new_dev_Y, new_dev_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "with open('train_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(train_losses)\n",
    "\n",
    "    \n",
    "with open('dev_loss.tsv', mode = 'w') as loss_file:\n",
    "        \n",
    "    csv_writer = csv.writer(loss_file)\n",
    "    csv_writer.writerow(dev_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(train_losses, ms = 4, marker = 's', label = \"Train Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(new_train_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('train_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "rc('text', usetex = True)\n",
    "rc('font', family='serif')\n",
    "plt.grid(True, ls = '-.',alpha = 0.4)\n",
    "plt.plot(dev_losses, ms = 4, marker = 'o', label = \"Dev Loss\")\n",
    "plt.legend(loc = \"best\")\n",
    "title = \"Cosine Similarity Loss (number of examples: \" + str(len(new_dev_X)) + \")\"\n",
    "plt.title(title)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Number of Iteration')\n",
    "plt.tight_layout()\n",
    "plt.savefig('dev_loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([-0.0487], grad_fn=<DivBackward0>), tensor([0.1172], grad_fn=<DivBackward0>), tensor([-0.0488], grad_fn=<DivBackward0>), tensor([-0.1072], grad_fn=<DivBackward0>), tensor([-0.0202], grad_fn=<DivBackward0>), tensor([-0.0275], grad_fn=<DivBackward0>), tensor([-0.0408], grad_fn=<DivBackward0>), tensor([-0.0774], grad_fn=<DivBackward0>)]\n",
      "result index: 1\n",
      "[tensor([-0.0487], grad_fn=<DivBackward0>), tensor([0.1172], grad_fn=<DivBackward0>), tensor([-0.0488], grad_fn=<DivBackward0>), tensor([-0.1073], grad_fn=<DivBackward0>), tensor([-0.0202], grad_fn=<DivBackward0>), tensor([-0.0275], grad_fn=<DivBackward0>), tensor([-0.0407], grad_fn=<DivBackward0>), tensor([-0.0774], grad_fn=<DivBackward0>)]\n",
      "result index: 1\n",
      "[tensor([-0.0487], grad_fn=<DivBackward0>), tensor([0.1172], grad_fn=<DivBackward0>), tensor([-0.0488], grad_fn=<DivBackward0>), tensor([-0.1073], grad_fn=<DivBackward0>), tensor([-0.0202], grad_fn=<DivBackward0>), tensor([-0.0275], grad_fn=<DivBackward0>), tensor([-0.0407], grad_fn=<DivBackward0>), tensor([-0.0774], grad_fn=<DivBackward0>)]\n",
      "result index: 1\n",
      "[tensor([-0.0487], grad_fn=<DivBackward0>), tensor([0.1172], grad_fn=<DivBackward0>), tensor([-0.0488], grad_fn=<DivBackward0>), tensor([-0.1073], grad_fn=<DivBackward0>), tensor([-0.0203], grad_fn=<DivBackward0>), tensor([-0.0275], grad_fn=<DivBackward0>), tensor([-0.0407], grad_fn=<DivBackward0>), tensor([-0.0774], grad_fn=<DivBackward0>)]\n",
      "result index: 1\n",
      "[tensor([-0.0487], grad_fn=<DivBackward0>), tensor([0.1172], grad_fn=<DivBackward0>), tensor([-0.0488], grad_fn=<DivBackward0>), tensor([-0.1073], grad_fn=<DivBackward0>), tensor([-0.0203], grad_fn=<DivBackward0>), tensor([-0.0275], grad_fn=<DivBackward0>), tensor([-0.0407], grad_fn=<DivBackward0>), tensor([-0.0774], grad_fn=<DivBackward0>)]\n",
      "result index: 1\n",
      "[tensor([-0.0487], grad_fn=<DivBackward0>), tensor([0.1172], grad_fn=<DivBackward0>), tensor([-0.0488], grad_fn=<DivBackward0>), tensor([-0.1073], grad_fn=<DivBackward0>), tensor([-0.0203], grad_fn=<DivBackward0>), tensor([-0.0275], grad_fn=<DivBackward0>), tensor([-0.0407], grad_fn=<DivBackward0>), tensor([-0.0774], grad_fn=<DivBackward0>)]\n",
      "result index: 1\n",
      "[tensor([-0.0487], grad_fn=<DivBackward0>), tensor([0.1170], grad_fn=<DivBackward0>), tensor([-0.0488], grad_fn=<DivBackward0>), tensor([-0.1076], grad_fn=<DivBackward0>), tensor([-0.0204], grad_fn=<DivBackward0>), tensor([-0.0272], grad_fn=<DivBackward0>), tensor([-0.0407], grad_fn=<DivBackward0>), tensor([-0.0772], grad_fn=<DivBackward0>)]\n",
      "result index: 1\n",
      "[tensor([-0.0487], grad_fn=<DivBackward0>), tensor([0.1172], grad_fn=<DivBackward0>), tensor([-0.0488], grad_fn=<DivBackward0>), tensor([-0.1073], grad_fn=<DivBackward0>), tensor([-0.0203], grad_fn=<DivBackward0>), tensor([-0.0275], grad_fn=<DivBackward0>), tensor([-0.0407], grad_fn=<DivBackward0>), tensor([-0.0773], grad_fn=<DivBackward0>)]\n",
      "result index: 1\n",
      "[tensor([-0.0487], grad_fn=<DivBackward0>), tensor([0.1172], grad_fn=<DivBackward0>), tensor([-0.0488], grad_fn=<DivBackward0>), tensor([-0.1073], grad_fn=<DivBackward0>), tensor([-0.0203], grad_fn=<DivBackward0>), tensor([-0.0275], grad_fn=<DivBackward0>), tensor([-0.0407], grad_fn=<DivBackward0>), tensor([-0.0774], grad_fn=<DivBackward0>)]\n",
      "result index: 1\n",
      "test size for known words: 9\n",
      "accuracy for known words: 0.5555555555555556\n",
      "test size for unknown words: 0\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "# modify to test only one word for now\n",
    "cos = nn.CosineSimilarity(dim = 0, eps = 1e-6)\n",
    "correct_count = 0\n",
    "known_test_size = 0\n",
    "unknown_test_size = 0\n",
    "unknown_correct_count = 0\n",
    "\n",
    "# overall accuracy\n",
    "for test_idx, test_sen in enumerate(new_test_X):\n",
    "    \n",
    "    test_lemma = test_sen[new_test_idx[test_idx]]\n",
    "    test_emb = trainer._model.forward(test_sen, new_test_idx[test_idx])\n",
    "    all_similarity = []\n",
    "    \n",
    "    # if it is a new word\n",
    "    # only test on the supersense\n",
    "    if new_all_senses.get(test_lemma, 'e') == 'e':\n",
    "        \n",
    "        unknown_test_size += 1\n",
    "        test_result = ''\n",
    "        best_sim = -float('inf')\n",
    "        \n",
    "        for n, new_s in enumerate(all_test_senses[test_lemma]):\n",
    "            \n",
    "            new_super = wn.synset(new_s).lexname().replace('.', '_')\n",
    "            super_vec = trainer._model.supersense_embeddings[new_super].view(trainer._model.output_size, -1)\n",
    "            cos_sim = cos(test_emb, super_vec)\n",
    "            \n",
    "            if cos_sim > best_sim:\n",
    "                test_result = new_super\n",
    "                best_sim = cos_sim\n",
    "                \n",
    "        correct_super = []\n",
    "        for q, respon in new_test_Y[test_idx]:\n",
    "            if respon:\n",
    "                correct_s = wn.synset(all_test_senses[test_lemma][q]).lexname().replace('.', '_')\n",
    "                correct_super.append(correct_s)            \n",
    "        if test_result in correct_super:\n",
    "            unknown_correct_count += 1\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # if it is a known word\n",
    "        known_test_size += 1\n",
    "        \n",
    "        for k, sense in enumerate(new_all_senses[test_lemma]):\n",
    "            definition_vec = trainer._model.definition_embeddings[test_lemma][:, k].view(trainer._model.output_size, -1)\n",
    "            cos_sim = cos(test_emb, definition_vec)\n",
    "            all_similarity.append(cos_sim)\n",
    "        # print(all_similarity)\n",
    "        test_result = all_similarity.index(max(all_similarity))\n",
    "        # print(\"result index: {}\".format(test_result))\n",
    "        if new_test_Y[test_idx][test_result] == 1:\n",
    "            correct_count += 1\n",
    "\n",
    "print('test size for known words: {}'.format(known_test_size))\n",
    "print('accuracy for known words: {}'.format(correct_count / known_test_size))\n",
    "\n",
    "print('test size for unknown words: {}'.format(unknown_test_size))\n",
    "# print('accuracy for unknown words: {}'.format(unknown_correct_count / unknown_test_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
